{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"imagededup Finding duplicates in an image dataset is a recurring task. imagededup is a python package that provides functionality to carry out this task effectively. The deduplication problem generally caters to 2 broad issues: Finding exact duplicates Finding near duplicates Traditional methods such as hashing algorithms are particularly good at finding exact duplicates while more modern methods involving convolutional neural networks are also adept at finding near duplicates due to their ability to capture basic contours in images. This package provides functionality to address both problems. Additionally, an evaluation framework is also provided to judge the quality of deduplication. Following details the functionality provided by the package: Finding duplicates in a directory using one of the following algorithms: Convolutional Neural Network Perceptual hashing Difference hashing Wavelet hashing Average hashing Generation of features for images using one of the above stated algorithms. Framework to evaluate effectiveness of deduplication given a ground truth mapping. Plotting duplicates found for a given image file. imagededup is compatible with Python 3.6 and is distributed under the Apache 2.0 license. Table of contents Installation Finding duplicates Feature generation Evaluation of deduplication Plotting duplicates Contribute Citation Maintainers License Installation There are two ways to install imagededup: Install imagededup from PyPI (recommended): pip install imagededup Install imagededup from the GitHub source: git clone https : // github . com / idealo / image - dedup . git cd image - dedup python setup . py install Getting started Finding duplicates There are two methods available to perform deduplication: find_duplicates() find_duplicates_to_remove() find_duplicates To deduplicate an image directory, the general api is: from imagededup.methods import < method - name > method_object = < method - name > () duplicates = method_object . find_duplicates ( image_dir = 'path/to/image/directory' , < threshold - parameter - value > ) where the returned variable duplicates is a dictionary with the following content: { 'image1.jpg' : [ 'image1_duplicate1.jpg' , 'image1_duplicate2.jpg' ], 'image2.jpg' : [..], .. } Each key in the duplicates dictionary corresponds to a file in the image directory passed to the image_dir parameter of the find_duplicates function. The value is a list of all file names in the image directory that were found to be duplicates for the key file. Options image_dir : Optional, directory where all image files are present. encoding_map : Optional, used instead of image_dir attribute. Set it equal to the dictionary of file names and corresponding features (hashes/cnn encodings). The mentioned dictionary can be generated using the corresponding encode_images method. scores : Setting it to True returns the scores representing the hamming distance (for hashing) or cosine similarity (for cnn) of each of the duplicate file names from the key file. In this case, the returned 'duplicates' dictionary has the following content: { 'image1.jpg' : [( 'image1_duplicate1.jpg' , score ), ( 'image1_duplicate2.jpg' , score )], 'image2.jpg' : [..], .. } Each key in the duplicates dictionary corresponds to a file in the image directory passed to the image_dir parameter of the find_duplicates function. The value is a list of all tuples representing the file names and corresponding scores in the image directory that were found to be duplicates for the key file. outfile : Name of file to which the returned duplicates dictionary is to be written. None by default. threshold parameter: min_similarity_threshold for cnn method indicating the minimum amount of cosine similarity that should exist between the key image and a candidate image so that the candidate image can be considered as a duplicate for the key image. Should be a float between -1.0 and 1.0. Default value is 0.9. max_distance_threshold for hashing methods indicating the maximum amount of hamming distance that can exist between the key image and a candidate image so that the candidate image can be considered as a duplicate for the key image. Should be an int between 0 and 64. Default value is 10. Considerations The returned duplicates dictionary contains symmetric relationships i.e., if an image i is a duplicate of image j , then image j must also be a duplicate of image i . Let's say that the image directory only consists of images i and j , then the duplicates dictionary would have the following content: { 'i' : [ 'j' ], 'j' : [ 'i' ] } If an image in the image directory can't be loaded, no features are generated for the image. Hence, the image is disregarded for deduplication and has no entry in the returned duplicates dictionary. Examples To deduplicate an image directory using perceptual hashing, with a maximum allowed hamming distance of 12, scores returned along with duplicate filenames and the returned dictionary saved to file 'my_duplicates.json', use the following: from imagededup.methods import PHash phasher = PHash () duplicates = phasher . find_duplicates ( image_dir = 'path/to/image/directory' , max_distance_threshold = 12 , scores = True , outfile = 'my_duplicates.json' ) To deduplicate an image directory using cnn, with a minimum cosine similarity of 0.85, no scores returned and the returned dictionary saved to file 'my_duplicates.json', use the following: from imagededup.methods import CNN cnn_encoder = CNN () duplicates = cnn_encoder . find_duplicates ( image_dir = 'path/to/image/directory' , min_similarity_threshold = 0.85 , scores = False , outfile = 'my_duplicates.json' ) find_duplicates_to_remove Returns a list of files in the image directory that are considered as duplicates. Does NOT remove the said files. The api is similar to find_duplicates function (except the score attribute in find_duplicates ). This function allows the return of a single list of file names in directory that are found to be duplicates. The general api for the method is as below: from imagededup.methods import < method - name > method_object = < method - name > () duplicates = method_object . find_duplicates_to_remove ( image_dir = 'path/to/image/directory' , < threshold - parameter - value > ) In this case, the returned variable duplicates is a list containing the name of image files that are found to be duplicates of some file in the directory: [ 'image1_duplicate1.jpg' , 'image1_duplicate2.jpg' ,.. ] Options image_dir : Optional, directory where all image files are present. encoding_map : Optional, used instead of image_dir attribute. Set it equal to the dictionary of file names and corresponding features (hashes/cnn encodings). The mentioned dictionary can be generated using the corresponding encode_images method. Each key in the 'duplicates' dictionary corresponds to a file in the image directory passed to the image_dir parameter of the find_duplicates function. The value is a list of all tuples representing the file names and corresponding scores in the image directory that were found to be duplicates for the key file. outfile : Name of file to which the returned duplicates dictionary is to be written. None by default. threshold parameter: min_similarity_threshold for cnn method indicating the minimum amount of cosine similarity that should exist between the key image and a candidate image so that the candidate image can be considered as a duplicate for the key image. Should be a float between -1.0 and 1.0. Default value is 0.9. max_distance_threshold for hashing methods indicating the maximum amount of hamming distance that can exist between the key image and a candidate image so that the candidate image can be considered as a duplicate for the key image. Should be an int between 0 and 64. Default value is 10. Considerations This method must be used with caution. The symmetric nature of duplicates imposes an issue of marking one image as duplicate and the other as original. Consider the following duplicates dictionary: { '1.jpg' : [ '2.jpg' ], '2.jpg' : [ '1.jpg' , '3.jpg' ], '3.jpg' : [ '2.jpg' ] } In this case, it is possible to remove only 2.jpg which leaves 1.jpg and 3.jpg as non-duplicates of each other. However, it is also possible to remove both 1.jpg and 3.jpg leaving only 2.jpg . The find_duplicates_to_remove method makes this decision based on the alphabetical sorting of filenames in the directory. In the above example, the filename 1.jpg appears alphabetically before 2.jpg . So, 1.jpg would be retained, while its duplicate, 2.jpg , would be marked as a duplicate. Once 2.jpg is marked as duplicate, its own found duplicates would be disregarded. Thus, 1.jpg and 3.jpg would not be considered as duplicates. So, the final return would be: [ '2.jpg' ] This leaves 1.jpg and 3.jpg as non-duplicates in the directory. If the user does not wish to impose this heuristic, it is advised to use find_duplicates function and use a custom heuristic to mark a file as duplicate. If an image in the image directory can't be loaded, no features are generated for the image. Hence, the image is disregarded for deduplication and has no entry in the returned duplicates dictionary. Examples To deduplicate an image directory using perceptual hashing, with a maximum allowed hamming distance of 12, and the returned list saved to file 'my_duplicates.json', use the following: from imagededup.methods import PHash phasher = PHash () duplicates = phasher . find_duplicates_to_remove ( image_dir = 'path/to/image/directory' , max_distance_threshold = 12 , outfile = 'my_duplicates.json' ) To deduplicate an image directory using cnn, with a minimum cosine similarity of 0.85 and the returned list saved to file 'my_duplicates.json', use the following: from imagededup.methods import CNN cnn_encoder = CNN () duplicates = cnn_encoder . find_duplicates_to_remove ( image_dir = 'path/to/image/directory' , min_similarity_threshold = 0.85 , outfile = 'my_duplicates.json' ) Feature generation It might be desirable to only generate the hashes/cnn encodings for a given image or all images in the directory instead of directly deduplicating using find_duplicates method. Features can be generated for a directory of images or for a single image: Feature generation for all images in a directory Feature generation for a single image Feature generation for all images in a directory To generate encodings for all images in an image directory encode_images function can be used. The general api for using encode_images is: from imagededup.methods import < method - name > method_object = < method - name > () encodings = method_object . encode_images ( image_dir = 'path/to/image/directory' ) where the returned variable encodings is a dictionary mapping image file names to corresponding encoding: { 'image1.jpg' : < feature - image - 1 > , 'image2.jpg' : < feature - image - 2 > , .. } For hashing algorithms, the features are 64 bit hashes represented as 16 character hexadecimal strings. For cnn, the features are numpy array with shape (1, 1024). Considerations If an image in the image directory can't be loaded, no features are generated for the image. Hence, there is no entry for the image in the returned encodings dictionary. Examples Generating features using Difference hash, from imagededup.methods import DHash dhasher = DHash () encodings = dhasher . encode_images ( image_dir = 'path/to/image/directory' ) Feature generation for a single image To generate encodings for a single image encode_image function can be used. The general api for using encode_image is: from imagededup.methods import < method - name > method_object = < method - name > () encoding = method_object . encode_image ( image_file = 'path/to/image/file' ) where the returned variable encoding is either a hexadecimal string if a hashing method is used or a (1, 1024) numpy array if cnn is used. Options image_file: Optional, path to the image file for which encodings are to be generated. image_array: Optional, used instead of image_file attribute. A numpy array representing the image. Considerations If the image can't be loaded, no features are generated for the image and None is returned. Examples Generating features using Difference hash, from imagededup.methods import DHash dhasher = DHash () encoding = dhasher . encode_image ( image_file = 'path/to/image/file' ) Evaluation of deduplication quality To determine the quality of deduplication algorithm and the corresponding threshold, an evaluation framework is provided. Given a ground truth mapping consisting of file names and a list of duplicates for each file along with a retrieved mapping from the deduplication algorithm for the same files, the following metrics can be obtained using the framework: Mean Average Precision (MAP) Mean Normalized Discounted Cumulative Gain (NDCG) Jaccard Index Per class Precision (class 0 = non-duplicate image pairs, class 1 = duplicate image pairs) Per class Recall (class 0 = non-duplicate image pairs, class 1 = duplicate image pairs) Per class f1-score (class 0 = non-duplicate image pairs, class 1 = duplicate image pairs) The api for obtaining these metrics is as below: from imagededup.evaluation import evaluate metrics = evaluate ( ground_truth_map , retrieved_map , metric = '<metric-name>' ) where the returned variable metrics is a dictionary containing the following content: { 'map' : < map > , 'ndcg' : < mean ndcg > , 'jaccard' : < mean jaccard index > , 'precision' : < numpy array having per class precision > , 'recall' : < numpy array having per class recall > , 'f1-score' : < numpy array having per class f1 - score > , 'support' : < numpy array having per class support > } Options ground_truth_map: A dictionary representing ground truth with filenames as key and a list of duplicate filenames as value. retrieved_map: A dictionary representing retrieved duplicates with filenames as key and a list of retrieved duplicate filenames as value. metric can take one of the following values: 'map' 'ndcg' 'jaccard' 'classification': Returns per class precision, recall, f1-score, support 'all' (default, returns all the above metrics) Considerations Presently, the ground truth map should be prepared manually by the user. Symmetric relations between duplicates must be represented in the ground truth map. If an image i is a duplicate for image j , then j must also be represented as a duplicate of i . Absence of symmetric relations will lead to an exception. Both the ground_truth_map and retrieved_map must have the same keys. Plotting duplicates of an image Once a duplicate dictionary corresponding to an image directory has been obtained (using find_duplicates ), duplicates for an image can be plotted using plot_duplicates method as below: from imagededup.utils import plot_duplicates plot_duplicates ( image_dir , duplicate_map , filename ) where filename is the file for which duplicates are to be plotted. Options image_dir : Directory where all image files are present. duplicate_map : A dictionary representing retrieved duplicates with filenames as key and a list of retrieved duplicate filenames as value. A duplicate_map with scores can also be passed (obtained from find_duplicates function with scores attribute set to True). filename : Image file name for which duplicates are to be plotted. outfile : Name of the file the plot should be saved to. None by default. The output looks as below: Contribute We welcome all kinds of contributions. See the Contribution guide for more details. Citation Please cite Imagededup in your publications if this is useful for your research. Here is an example BibTeX entry: @misc { idealods2019imagededup , title = { Imagededup } , author = { Tanuj Jain and Christopher Lennan and Zubin John } , year = { 2019 } , howpublished = {\\ url { https : // github . com / idealo / image - dedup }} , } Maintainers Tanuj Jain, github: tanujjain Christopher Lennan, github: clennan Copyright See LICENSE for details.","title":"Home"},{"location":"#imagededup","text":"Finding duplicates in an image dataset is a recurring task. imagededup is a python package that provides functionality to carry out this task effectively. The deduplication problem generally caters to 2 broad issues: Finding exact duplicates Finding near duplicates Traditional methods such as hashing algorithms are particularly good at finding exact duplicates while more modern methods involving convolutional neural networks are also adept at finding near duplicates due to their ability to capture basic contours in images. This package provides functionality to address both problems. Additionally, an evaluation framework is also provided to judge the quality of deduplication. Following details the functionality provided by the package: Finding duplicates in a directory using one of the following algorithms: Convolutional Neural Network Perceptual hashing Difference hashing Wavelet hashing Average hashing Generation of features for images using one of the above stated algorithms. Framework to evaluate effectiveness of deduplication given a ground truth mapping. Plotting duplicates found for a given image file. imagededup is compatible with Python 3.6 and is distributed under the Apache 2.0 license.","title":"imagededup"},{"location":"#table-of-contents","text":"Installation Finding duplicates Feature generation Evaluation of deduplication Plotting duplicates Contribute Citation Maintainers License","title":"Table of contents"},{"location":"#installation","text":"There are two ways to install imagededup: Install imagededup from PyPI (recommended): pip install imagededup Install imagededup from the GitHub source: git clone https : // github . com / idealo / image - dedup . git cd image - dedup python setup . py install","title":"Installation"},{"location":"#getting-started","text":"","title":"Getting started"},{"location":"#finding-duplicates","text":"There are two methods available to perform deduplication: find_duplicates() find_duplicates_to_remove()","title":"Finding duplicates"},{"location":"#find_duplicates","text":"To deduplicate an image directory, the general api is: from imagededup.methods import < method - name > method_object = < method - name > () duplicates = method_object . find_duplicates ( image_dir = 'path/to/image/directory' , < threshold - parameter - value > ) where the returned variable duplicates is a dictionary with the following content: { 'image1.jpg' : [ 'image1_duplicate1.jpg' , 'image1_duplicate2.jpg' ], 'image2.jpg' : [..], .. } Each key in the duplicates dictionary corresponds to a file in the image directory passed to the image_dir parameter of the find_duplicates function. The value is a list of all file names in the image directory that were found to be duplicates for the key file.","title":"find_duplicates"},{"location":"#options","text":"image_dir : Optional, directory where all image files are present. encoding_map : Optional, used instead of image_dir attribute. Set it equal to the dictionary of file names and corresponding features (hashes/cnn encodings). The mentioned dictionary can be generated using the corresponding encode_images method. scores : Setting it to True returns the scores representing the hamming distance (for hashing) or cosine similarity (for cnn) of each of the duplicate file names from the key file. In this case, the returned 'duplicates' dictionary has the following content: { 'image1.jpg' : [( 'image1_duplicate1.jpg' , score ), ( 'image1_duplicate2.jpg' , score )], 'image2.jpg' : [..], .. } Each key in the duplicates dictionary corresponds to a file in the image directory passed to the image_dir parameter of the find_duplicates function. The value is a list of all tuples representing the file names and corresponding scores in the image directory that were found to be duplicates for the key file. outfile : Name of file to which the returned duplicates dictionary is to be written. None by default. threshold parameter: min_similarity_threshold for cnn method indicating the minimum amount of cosine similarity that should exist between the key image and a candidate image so that the candidate image can be considered as a duplicate for the key image. Should be a float between -1.0 and 1.0. Default value is 0.9. max_distance_threshold for hashing methods indicating the maximum amount of hamming distance that can exist between the key image and a candidate image so that the candidate image can be considered as a duplicate for the key image. Should be an int between 0 and 64. Default value is 10.","title":"Options"},{"location":"#considerations","text":"The returned duplicates dictionary contains symmetric relationships i.e., if an image i is a duplicate of image j , then image j must also be a duplicate of image i . Let's say that the image directory only consists of images i and j , then the duplicates dictionary would have the following content: { 'i' : [ 'j' ], 'j' : [ 'i' ] } If an image in the image directory can't be loaded, no features are generated for the image. Hence, the image is disregarded for deduplication and has no entry in the returned duplicates dictionary.","title":"Considerations"},{"location":"#examples","text":"To deduplicate an image directory using perceptual hashing, with a maximum allowed hamming distance of 12, scores returned along with duplicate filenames and the returned dictionary saved to file 'my_duplicates.json', use the following: from imagededup.methods import PHash phasher = PHash () duplicates = phasher . find_duplicates ( image_dir = 'path/to/image/directory' , max_distance_threshold = 12 , scores = True , outfile = 'my_duplicates.json' ) To deduplicate an image directory using cnn, with a minimum cosine similarity of 0.85, no scores returned and the returned dictionary saved to file 'my_duplicates.json', use the following: from imagededup.methods import CNN cnn_encoder = CNN () duplicates = cnn_encoder . find_duplicates ( image_dir = 'path/to/image/directory' , min_similarity_threshold = 0.85 , scores = False , outfile = 'my_duplicates.json' )","title":"Examples"},{"location":"#find_duplicates_to_remove","text":"Returns a list of files in the image directory that are considered as duplicates. Does NOT remove the said files. The api is similar to find_duplicates function (except the score attribute in find_duplicates ). This function allows the return of a single list of file names in directory that are found to be duplicates. The general api for the method is as below: from imagededup.methods import < method - name > method_object = < method - name > () duplicates = method_object . find_duplicates_to_remove ( image_dir = 'path/to/image/directory' , < threshold - parameter - value > ) In this case, the returned variable duplicates is a list containing the name of image files that are found to be duplicates of some file in the directory: [ 'image1_duplicate1.jpg' , 'image1_duplicate2.jpg' ,.. ]","title":"find_duplicates_to_remove"},{"location":"#options_1","text":"image_dir : Optional, directory where all image files are present. encoding_map : Optional, used instead of image_dir attribute. Set it equal to the dictionary of file names and corresponding features (hashes/cnn encodings). The mentioned dictionary can be generated using the corresponding encode_images method. Each key in the 'duplicates' dictionary corresponds to a file in the image directory passed to the image_dir parameter of the find_duplicates function. The value is a list of all tuples representing the file names and corresponding scores in the image directory that were found to be duplicates for the key file. outfile : Name of file to which the returned duplicates dictionary is to be written. None by default. threshold parameter: min_similarity_threshold for cnn method indicating the minimum amount of cosine similarity that should exist between the key image and a candidate image so that the candidate image can be considered as a duplicate for the key image. Should be a float between -1.0 and 1.0. Default value is 0.9. max_distance_threshold for hashing methods indicating the maximum amount of hamming distance that can exist between the key image and a candidate image so that the candidate image can be considered as a duplicate for the key image. Should be an int between 0 and 64. Default value is 10.","title":"Options"},{"location":"#considerations_1","text":"This method must be used with caution. The symmetric nature of duplicates imposes an issue of marking one image as duplicate and the other as original. Consider the following duplicates dictionary: { '1.jpg' : [ '2.jpg' ], '2.jpg' : [ '1.jpg' , '3.jpg' ], '3.jpg' : [ '2.jpg' ] } In this case, it is possible to remove only 2.jpg which leaves 1.jpg and 3.jpg as non-duplicates of each other. However, it is also possible to remove both 1.jpg and 3.jpg leaving only 2.jpg . The find_duplicates_to_remove method makes this decision based on the alphabetical sorting of filenames in the directory. In the above example, the filename 1.jpg appears alphabetically before 2.jpg . So, 1.jpg would be retained, while its duplicate, 2.jpg , would be marked as a duplicate. Once 2.jpg is marked as duplicate, its own found duplicates would be disregarded. Thus, 1.jpg and 3.jpg would not be considered as duplicates. So, the final return would be: [ '2.jpg' ] This leaves 1.jpg and 3.jpg as non-duplicates in the directory. If the user does not wish to impose this heuristic, it is advised to use find_duplicates function and use a custom heuristic to mark a file as duplicate. If an image in the image directory can't be loaded, no features are generated for the image. Hence, the image is disregarded for deduplication and has no entry in the returned duplicates dictionary.","title":"Considerations"},{"location":"#examples_1","text":"To deduplicate an image directory using perceptual hashing, with a maximum allowed hamming distance of 12, and the returned list saved to file 'my_duplicates.json', use the following: from imagededup.methods import PHash phasher = PHash () duplicates = phasher . find_duplicates_to_remove ( image_dir = 'path/to/image/directory' , max_distance_threshold = 12 , outfile = 'my_duplicates.json' ) To deduplicate an image directory using cnn, with a minimum cosine similarity of 0.85 and the returned list saved to file 'my_duplicates.json', use the following: from imagededup.methods import CNN cnn_encoder = CNN () duplicates = cnn_encoder . find_duplicates_to_remove ( image_dir = 'path/to/image/directory' , min_similarity_threshold = 0.85 , outfile = 'my_duplicates.json' )","title":"Examples"},{"location":"#feature-generation","text":"It might be desirable to only generate the hashes/cnn encodings for a given image or all images in the directory instead of directly deduplicating using find_duplicates method. Features can be generated for a directory of images or for a single image: Feature generation for all images in a directory Feature generation for a single image","title":"Feature generation"},{"location":"#feature-generation-for-all-images-in-a-directory","text":"To generate encodings for all images in an image directory encode_images function can be used. The general api for using encode_images is: from imagededup.methods import < method - name > method_object = < method - name > () encodings = method_object . encode_images ( image_dir = 'path/to/image/directory' ) where the returned variable encodings is a dictionary mapping image file names to corresponding encoding: { 'image1.jpg' : < feature - image - 1 > , 'image2.jpg' : < feature - image - 2 > , .. } For hashing algorithms, the features are 64 bit hashes represented as 16 character hexadecimal strings. For cnn, the features are numpy array with shape (1, 1024).","title":"Feature generation for all images in a directory"},{"location":"#considerations_2","text":"If an image in the image directory can't be loaded, no features are generated for the image. Hence, there is no entry for the image in the returned encodings dictionary.","title":"Considerations"},{"location":"#examples_2","text":"Generating features using Difference hash, from imagededup.methods import DHash dhasher = DHash () encodings = dhasher . encode_images ( image_dir = 'path/to/image/directory' )","title":"Examples"},{"location":"#feature-generation-for-a-single-image","text":"To generate encodings for a single image encode_image function can be used. The general api for using encode_image is: from imagededup.methods import < method - name > method_object = < method - name > () encoding = method_object . encode_image ( image_file = 'path/to/image/file' ) where the returned variable encoding is either a hexadecimal string if a hashing method is used or a (1, 1024) numpy array if cnn is used.","title":"Feature generation for a single image"},{"location":"#options_2","text":"image_file: Optional, path to the image file for which encodings are to be generated. image_array: Optional, used instead of image_file attribute. A numpy array representing the image.","title":"Options"},{"location":"#considerations_3","text":"If the image can't be loaded, no features are generated for the image and None is returned.","title":"Considerations"},{"location":"#examples_3","text":"Generating features using Difference hash, from imagededup.methods import DHash dhasher = DHash () encoding = dhasher . encode_image ( image_file = 'path/to/image/file' )","title":"Examples"},{"location":"#evaluation-of-deduplication-quality","text":"To determine the quality of deduplication algorithm and the corresponding threshold, an evaluation framework is provided. Given a ground truth mapping consisting of file names and a list of duplicates for each file along with a retrieved mapping from the deduplication algorithm for the same files, the following metrics can be obtained using the framework: Mean Average Precision (MAP) Mean Normalized Discounted Cumulative Gain (NDCG) Jaccard Index Per class Precision (class 0 = non-duplicate image pairs, class 1 = duplicate image pairs) Per class Recall (class 0 = non-duplicate image pairs, class 1 = duplicate image pairs) Per class f1-score (class 0 = non-duplicate image pairs, class 1 = duplicate image pairs) The api for obtaining these metrics is as below: from imagededup.evaluation import evaluate metrics = evaluate ( ground_truth_map , retrieved_map , metric = '<metric-name>' ) where the returned variable metrics is a dictionary containing the following content: { 'map' : < map > , 'ndcg' : < mean ndcg > , 'jaccard' : < mean jaccard index > , 'precision' : < numpy array having per class precision > , 'recall' : < numpy array having per class recall > , 'f1-score' : < numpy array having per class f1 - score > , 'support' : < numpy array having per class support > }","title":"Evaluation of deduplication quality"},{"location":"#options_3","text":"ground_truth_map: A dictionary representing ground truth with filenames as key and a list of duplicate filenames as value. retrieved_map: A dictionary representing retrieved duplicates with filenames as key and a list of retrieved duplicate filenames as value. metric can take one of the following values: 'map' 'ndcg' 'jaccard' 'classification': Returns per class precision, recall, f1-score, support 'all' (default, returns all the above metrics)","title":"Options"},{"location":"#considerations_4","text":"Presently, the ground truth map should be prepared manually by the user. Symmetric relations between duplicates must be represented in the ground truth map. If an image i is a duplicate for image j , then j must also be represented as a duplicate of i . Absence of symmetric relations will lead to an exception. Both the ground_truth_map and retrieved_map must have the same keys.","title":"Considerations"},{"location":"#plotting-duplicates-of-an-image","text":"Once a duplicate dictionary corresponding to an image directory has been obtained (using find_duplicates ), duplicates for an image can be plotted using plot_duplicates method as below: from imagededup.utils import plot_duplicates plot_duplicates ( image_dir , duplicate_map , filename ) where filename is the file for which duplicates are to be plotted.","title":"Plotting duplicates of an image"},{"location":"#options_4","text":"image_dir : Directory where all image files are present. duplicate_map : A dictionary representing retrieved duplicates with filenames as key and a list of retrieved duplicate filenames as value. A duplicate_map with scores can also be passed (obtained from find_duplicates function with scores attribute set to True). filename : Image file name for which duplicates are to be plotted. outfile : Name of the file the plot should be saved to. None by default. The output looks as below:","title":"Options"},{"location":"#contribute","text":"We welcome all kinds of contributions. See the Contribution guide for more details.","title":"Contribute"},{"location":"#citation","text":"Please cite Imagededup in your publications if this is useful for your research. Here is an example BibTeX entry: @misc { idealods2019imagededup , title = { Imagededup } , author = { Tanuj Jain and Christopher Lennan and Zubin John } , year = { 2019 } , howpublished = {\\ url { https : // github . com / idealo / image - dedup }} , }","title":"Citation"},{"location":"#maintainers","text":"Tanuj Jain, github: tanujjain Christopher Lennan, github: clennan","title":"Maintainers"},{"location":"#copyright","text":"See LICENSE for details.","title":"Copyright"},{"location":"CONTRIBUTING/","text":"Contribution Guide We welcome any contributions whether it's, Submitting feedback Fixing bugs Or implementing a new feature. Please read this guide before making any contributions. Submit Feedback The feedback should be submitted by creating an issue at GitHub issues . Select the related template (bug report, feature request, or custom) and add the corresponding labels. Fix Bugs: You may look through the GitHub issues for bugs. Implement Features You may look through the GitHub issues for feature requests. Pull Requests (PR) Fork the repository and a create a new branch from the master branch. For bug fixes, add new tests and for new features please add changes to the documentation. Do a PR from your new branch to our dev branch of the original Image Super-Resolution repo. Documentation Make sure any new function or class you introduce has proper docstrings. Testing We use pytest for our testing. Make sure to write tests for any new feature and/or bug fixes. Main Contributor List We maintain a list of main contributors to appreciate all the contributions.","title":"Contribution"},{"location":"CONTRIBUTING/#contribution-guide","text":"We welcome any contributions whether it's, Submitting feedback Fixing bugs Or implementing a new feature. Please read this guide before making any contributions.","title":"Contribution Guide"},{"location":"CONTRIBUTING/#submit-feedback","text":"The feedback should be submitted by creating an issue at GitHub issues . Select the related template (bug report, feature request, or custom) and add the corresponding labels.","title":"Submit Feedback"},{"location":"CONTRIBUTING/#fix-bugs","text":"You may look through the GitHub issues for bugs.","title":"Fix Bugs:"},{"location":"CONTRIBUTING/#implement-features","text":"You may look through the GitHub issues for feature requests.","title":"Implement Features"},{"location":"CONTRIBUTING/#pull-requests-pr","text":"Fork the repository and a create a new branch from the master branch. For bug fixes, add new tests and for new features please add changes to the documentation. Do a PR from your new branch to our dev branch of the original Image Super-Resolution repo.","title":"Pull Requests (PR)"},{"location":"CONTRIBUTING/#documentation","text":"Make sure any new function or class you introduce has proper docstrings.","title":"Documentation"},{"location":"CONTRIBUTING/#testing","text":"We use pytest for our testing. Make sure to write tests for any new feature and/or bug fixes.","title":"Testing"},{"location":"CONTRIBUTING/#main-contributor-list","text":"We maintain a list of main contributors to appreciate all the contributions.","title":"Main Contributor List"},{"location":"LICENSE/","text":"Copyright 2019 idealo internet GmbH. All rights reserved. Apache License Version 2 . 0 , January 2004 http : // www . apache . org / licenses / TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION Definitions. \"License\" shall mean the terms and conditions for use, reproduction, and distribution as defined by Sections 1 through 9 of this document. \"Licensor\" shall mean the copyright owner or entity authorized by the copyright owner that is granting the License. \"Legal Entity\" shall mean the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. For the purposes of this definition, \"control\" means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity. \"You\" (or \"Your\") shall mean an individual or Legal Entity exercising permissions granted by this License. \"Source\" form shall mean the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files. \"Object\" form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation, and conversions to other media types. \"Work\" shall mean the work of authorship, whether in Source or Object form, made available under the License, as indicated by a copyright notice that is included in or attached to the work (an example is provided in the Appendix below). \"Derivative Works\" shall mean any work, whether in Source or Object form, that is based on (or derived from) the Work and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. For the purposes of this License, Derivative Works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof. \"Contribution\" shall mean any work of authorship, including the original version of the Work and any modifications or additions to that Work or Derivative Works thereof, that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes of this definition, \"submitted\" means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as \"Not a Contribution.\" \"Contributor\" shall mean Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Work. Grant of Copyright License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form. Grant of Patent License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s) with the Work to which such Contribution(s) was submitted. If You institute patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Work or a Contribution incorporated within the Work constitutes direct or contributory patent infringement, then any patent licenses granted to You under this License for that Work shall terminate as of the date such litigation is filed. Redistribution. You may reproduce and distribute copies of the Work or Derivative Works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions: (a) You must give any other recipients of the Work or Derivative Works a copy of this License; and (b) You must cause any modified files to carry prominent notices stating that You changed the files; and (c) You must retain, in the Source form of any Derivative Works that You distribute, all copyright, patent, trademark, and attribution notices from the Source form of the Work, excluding those notices that do not pertain to any part of the Derivative Works; and (d) If the Work includes a \"NOTICE\" text file as part of its distribution, then any Derivative Works that You distribute must include a readable copy of the attribution notices contained within such NOTICE file, excluding those notices that do not pertain to any part of the Derivative Works, in at least one of the following places: within a NOTICE text file distributed as part of the Derivative Works; within the Source form or documentation, if provided along with the Derivative Works; or, within a display generated by the Derivative Works, if and wherever such third-party notices normally appear. The contents of the NOTICE file are for informational purposes only and do not modify the License. You may add Your own attribution notices within Derivative Works that You distribute, alongside or as an addendum to the NOTICE text from the Work, provided that such additional attribution notices cannot be construed as modifying the License. You may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such Derivative Works as a whole, provided Your use, reproduction, and distribution of the Work otherwise complies with the conditions stated in this License. Submission of Contributions. Unless You explicitly state otherwise, any Contribution intentionally submitted for inclusion in the Work by You to the Licensor shall be under the terms and conditions of this License, without any additional terms or conditions. Notwithstanding the above, nothing herein shall supersede or modify the terms of any separate license agreement you may have executed with Licensor regarding such Contributions. Trademarks. This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and reproducing the content of the NOTICE file. Disclaimer of Warranty. Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions) on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License. Limitation of Liability. In no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any Contributor be liable to You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this License or out of the use or inability to use the Work (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if such Contributor has been advised of the possibility of such damages. Accepting Warranty or Additional Liability. While redistributing the Work or Derivative Works thereof, You may choose to offer, and charge a fee for, acceptance of support, warranty, indemnity, or other liability obligations and/or rights consistent with this License. However, in accepting such obligations, You may act only on Your own behalf and on Your sole responsibility, not on behalf of any other Contributor, and only if You agree to indemnify, defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against, such Contributor by reason of your accepting any such warranty or additional liability. END OF TERMS AND CONDITIONS APPENDIX: How to apply the Apache License to your work. To apply the Apache License to your work , attach the following boilerplate notice , with the fields enclosed by brackets \" [] \" replaced with your own identifying information . ( Don ' t include the brackets ! ) The text should be enclosed in the appropriate comment syntax for the file format . We also recommend that a file or class name and description of purpose be included on the same \" printed page \" as the copyright notice for easier identification within third - party archives . Copyright [yyyy] [name of copyright owner] Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http : // www . apache . org / licenses / LICENSE - 2 . 0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"start/","text":"Getting started","title":"Start"},{"location":"start/#getting-started","text":"","title":"Getting started"},{"location":"evaluation/evaluation/","text":"evaluate def evaluate ( ground_truth_map , retrieved_map , metric ) Given a ground truth map and a duplicate map retrieved from a deduplication algorithm, get metrics to evaluate the effectiveness of the applied deduplication algorithm. Args ground_truth_map : A dictionary representing ground truth with filenames as key and a list of duplicate filenames as value. retrieved_map : A dictionary representing retrieved duplicates with filenames as key and a list of retrieved duplicate filenames as value. metric : Name of metric to be evaluated and returned. Accepted values are: 'map', 'ndcg', 'jaccard', 'classification', 'all'(returns every metric). Returns dictionary : A dictionary with metric name as key and corresponding calculated metric as the value. 'map', 'ndcg' and 'jaccard' return a single number denoting the corresponding information retrieval metric. 'classification' metrics include 'precision', 'recall' and 'f1-score' which are returned in the form of individual entries in the returned dictionary. The value for each of the classification metric is a numpy array with first entry as the score for non-duplicate file pairs(class-0) and second entry as the score for duplicate file pairs (class-1). Additionally, a support is also returned as another key with first entry denoting number of non-duplicate file pairs and second entry having duplicate file pairs.","title":"Evaluation"},{"location":"evaluation/evaluation/#evaluate","text":"def evaluate ( ground_truth_map , retrieved_map , metric ) Given a ground truth map and a duplicate map retrieved from a deduplication algorithm, get metrics to evaluate the effectiveness of the applied deduplication algorithm.","title":"evaluate"},{"location":"evaluation/evaluation/#args","text":"ground_truth_map : A dictionary representing ground truth with filenames as key and a list of duplicate filenames as value. retrieved_map : A dictionary representing retrieved duplicates with filenames as key and a list of retrieved duplicate filenames as value. metric : Name of metric to be evaluated and returned. Accepted values are: 'map', 'ndcg', 'jaccard', 'classification', 'all'(returns every metric).","title":"Args"},{"location":"evaluation/evaluation/#returns","text":"dictionary : A dictionary with metric name as key and corresponding calculated metric as the value. 'map', 'ndcg' and 'jaccard' return a single number denoting the corresponding information retrieval metric. 'classification' metrics include 'precision', 'recall' and 'f1-score' which are returned in the form of individual entries in the returned dictionary. The value for each of the classification metric is a numpy array with first entry as the score for non-duplicate file pairs(class-0) and second entry as the score for duplicate file pairs (class-1). Additionally, a support is also returned as another key with first entry denoting number of non-duplicate file pairs and second entry having duplicate file pairs.","title":"Returns"},{"location":"examples/cats_and_dogs/","text":"Cats and Dogs Example Install imageatm via PyPi pip install imageatm Download the cats and dogs dataset wget --no-check-certificate \\ https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip \\ -O cats_and_dogs_filtered.zip Unzip dataset and create working directory unzip cats_and_dogs_filtered.zip mkdir -p cats_and_dogs/train mv cats_and_dogs_filtered/train/cats/* cats_and_dogs/train mv cats_and_dogs_filtered/train/dogs/* cats_and_dogs/train Create the sample file import os import json filenames = os . listdir ( 'cats_and_dogs/train' ) sample_json = [] for i in filenames : sample_json . append ( { 'image_id' : i , 'label' : 'Cat' if 'cat' in i else 'Dog' } ) with open ( 'data.json' , 'w' ) as outfile : json . dump ( sample_json , outfile , indent = 4 , sort_keys = True ) Run the data preparation with resizing from imageatm.components import DataPrep dp = DataPrep ( image_dir = 'cats_and_dogs/train' , samples_file = 'data.json' , job_dir = 'cats_and_dogs' ) dp . run ( resize = True ) Initialize the Training class and run it from imageatm.components import Training trainer = Training ( dp . image_dir , dp . job_dir , epochs_train_dense = 5 , epochs_train_all = 5 ) trainer . run () Evaluate the best model from imageatm.components import Evaluation e = Evaluation ( image_dir = dp . image_dir , job_dir = dp . job_dir ) e . run () Visualize CAM analysis on the correct and wrong examples c , w = e . get_correct_wrong_examples ( label = 1 ) e . visualize_images ( w , show_heatmap = True ) e . visualize_images ( c , show_heatmap = True )","title":"Cats and Dogs Example"},{"location":"examples/cats_and_dogs/#cats-and-dogs-example","text":"","title":"Cats and Dogs Example"},{"location":"examples/cats_and_dogs/#install-imageatm-via-pypi","text":"pip install imageatm","title":"Install imageatm via PyPi"},{"location":"examples/cats_and_dogs/#download-the-cats-and-dogs-dataset","text":"wget --no-check-certificate \\ https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip \\ -O cats_and_dogs_filtered.zip","title":"Download the cats and dogs dataset"},{"location":"examples/cats_and_dogs/#unzip-dataset-and-create-working-directory","text":"unzip cats_and_dogs_filtered.zip mkdir -p cats_and_dogs/train mv cats_and_dogs_filtered/train/cats/* cats_and_dogs/train mv cats_and_dogs_filtered/train/dogs/* cats_and_dogs/train","title":"Unzip dataset and create working directory"},{"location":"examples/cats_and_dogs/#create-the-sample-file","text":"import os import json filenames = os . listdir ( 'cats_and_dogs/train' ) sample_json = [] for i in filenames : sample_json . append ( { 'image_id' : i , 'label' : 'Cat' if 'cat' in i else 'Dog' } ) with open ( 'data.json' , 'w' ) as outfile : json . dump ( sample_json , outfile , indent = 4 , sort_keys = True )","title":"Create the sample file"},{"location":"examples/cats_and_dogs/#run-the-data-preparation-with-resizing","text":"from imageatm.components import DataPrep dp = DataPrep ( image_dir = 'cats_and_dogs/train' , samples_file = 'data.json' , job_dir = 'cats_and_dogs' ) dp . run ( resize = True )","title":"Run the data preparation with resizing"},{"location":"examples/cats_and_dogs/#initialize-the-training-class-and-run-it","text":"from imageatm.components import Training trainer = Training ( dp . image_dir , dp . job_dir , epochs_train_dense = 5 , epochs_train_all = 5 ) trainer . run ()","title":"Initialize the Training class and run it"},{"location":"examples/cats_and_dogs/#evaluate-the-best-model","text":"from imageatm.components import Evaluation e = Evaluation ( image_dir = dp . image_dir , job_dir = dp . job_dir ) e . run ()","title":"Evaluate the best model"},{"location":"examples/cats_and_dogs/#visualize-cam-analysis-on-the-correct-and-wrong-examples","text":"c , w = e . get_correct_wrong_examples ( label = 1 ) e . visualize_images ( w , show_heatmap = True ) e . visualize_images ( c , show_heatmap = True )","title":"Visualize CAM analysis on the correct and wrong examples"},{"location":"examples/imagenette/","text":"Imagenette Example Install imageatm via PyPi pip install imageatm Download the Imagenette dataset (320px) and ImageNet mapping wget --no-check-certificate \\ https://s3.amazonaws.com/fast-ai-imageclas/imagenette-320.tgz wget --no-check-certificate \\ https://raw.githubusercontent.com/ozendelait/wordnet-to-json/master/mapping_imagenet.json Untar the dataset tar -xzf imagenette-320.tgz Create mapping for Imagenette classes and prepare the data.json import os import json def load_json ( file_path ): with open ( file_path , 'r' ) as f : return json . load ( f ) mapping = load_json ( 'mapping_imagenet.json' ) mapping_synset_txt = {} for i , j in enumerate ( mapping ): mapping_synset_txt [ j [ 'v3p0' ]] = j [ 'label' ] . split ( ',' )[ 0 ] classes = os . listdir ( 'imagenette-320/train' ) sample_json = [] for c in classes : filenames = os . listdir ( 'imagenette-320/train/{}' . format ( c )) for i in filenames : sample_json . append ( { 'image_id' : i , 'label' : mapping_synset_txt [ c ] } ) with open ( 'data.json' , 'w' ) as outfile : json . dump ( sample_json , outfile , indent = 4 , sort_keys = True ) Prepare our image directory IMAGE_DIR = 'images' if not os . path . exists ( IMAGE_DIR ): os . makedirs ( IMAGE_DIR ) classes = os . listdir ( 'imagenette-320/train' ) for c in classes : cmd = 'cp -r {}. {}' . format ( os . path . join ( 'imagenette-320/train' , c ) + '/' , os . path . join ( IMAGE_DIR )) os . system ( cmd ) Run the data preparation from imageatm.components import DataPrep dp = DataPrep ( image_dir = 'images' , samples_file = 'data.json' , job_dir = 'imagenette' ) dp . run ( resize = False ) Initialize the Training class and run it from imageatm.components import Training trainer = Training ( dp . image_dir , dp . job_dir , epochs_train_dense = 5 , epochs_train_all = 5 , batch_size = 64 , ) trainer . run () Evaluate the best model from imageatm.components import Evaluation e = Evaluation ( image_dir = dp . image_dir , job_dir = dp . job_dir ) e . run () Visualize CAM analysis on the correct and wrong examples c , w = e . get_correct_wrong_examples ( label = 1 ) e . visualize_images ( w , show_heatmap = True ) e . visualize_images ( c , show_heatmap = True )","title":"Imagenette Example"},{"location":"examples/imagenette/#imagenette-example","text":"","title":"Imagenette Example"},{"location":"examples/imagenette/#install-imageatm-via-pypi","text":"pip install imageatm","title":"Install imageatm via PyPi"},{"location":"examples/imagenette/#download-the-imagenette-dataset-320px-and-imagenet-mapping","text":"wget --no-check-certificate \\ https://s3.amazonaws.com/fast-ai-imageclas/imagenette-320.tgz wget --no-check-certificate \\ https://raw.githubusercontent.com/ozendelait/wordnet-to-json/master/mapping_imagenet.json","title":"Download the Imagenette dataset (320px) and ImageNet mapping"},{"location":"examples/imagenette/#untar-the-dataset","text":"tar -xzf imagenette-320.tgz","title":"Untar the dataset"},{"location":"examples/imagenette/#create-mapping-for-imagenette-classes-and-prepare-the-datajson","text":"import os import json def load_json ( file_path ): with open ( file_path , 'r' ) as f : return json . load ( f ) mapping = load_json ( 'mapping_imagenet.json' ) mapping_synset_txt = {} for i , j in enumerate ( mapping ): mapping_synset_txt [ j [ 'v3p0' ]] = j [ 'label' ] . split ( ',' )[ 0 ] classes = os . listdir ( 'imagenette-320/train' ) sample_json = [] for c in classes : filenames = os . listdir ( 'imagenette-320/train/{}' . format ( c )) for i in filenames : sample_json . append ( { 'image_id' : i , 'label' : mapping_synset_txt [ c ] } ) with open ( 'data.json' , 'w' ) as outfile : json . dump ( sample_json , outfile , indent = 4 , sort_keys = True )","title":"Create mapping for Imagenette classes and prepare the data.json"},{"location":"examples/imagenette/#prepare-our-image-directory","text":"IMAGE_DIR = 'images' if not os . path . exists ( IMAGE_DIR ): os . makedirs ( IMAGE_DIR ) classes = os . listdir ( 'imagenette-320/train' ) for c in classes : cmd = 'cp -r {}. {}' . format ( os . path . join ( 'imagenette-320/train' , c ) + '/' , os . path . join ( IMAGE_DIR )) os . system ( cmd )","title":"Prepare our image directory"},{"location":"examples/imagenette/#run-the-data-preparation","text":"from imageatm.components import DataPrep dp = DataPrep ( image_dir = 'images' , samples_file = 'data.json' , job_dir = 'imagenette' ) dp . run ( resize = False )","title":"Run the data preparation"},{"location":"examples/imagenette/#initialize-the-training-class-and-run-it","text":"from imageatm.components import Training trainer = Training ( dp . image_dir , dp . job_dir , epochs_train_dense = 5 , epochs_train_all = 5 , batch_size = 64 , ) trainer . run ()","title":"Initialize the Training class and run it"},{"location":"examples/imagenette/#evaluate-the-best-model","text":"from imageatm.components import Evaluation e = Evaluation ( image_dir = dp . image_dir , job_dir = dp . job_dir ) e . run ()","title":"Evaluate the best model"},{"location":"examples/imagenette/#visualize-cam-analysis-on-the-correct-and-wrong-examples","text":"c , w = e . get_correct_wrong_examples ( label = 1 ) e . visualize_images ( w , show_heatmap = True ) e . visualize_images ( c , show_heatmap = True )","title":"Visualize CAM analysis on the correct and wrong examples"},{"location":"handlers/metrics/classification/","text":"classification_metrics def classification_metrics ( ground_truth , retrieved ) Given ground truth dictionary and retrieved dictionary, return per class precision, recall and f1 score. Class 1 is assigned to duplicate file pairs while class 0 is for non-duplicate file pairs. Args ground_truth : A dictionary representing ground truth with filenames as key and a list of duplicate filenames retrieved : A dictionary representing retrieved duplicates with filenames as key and a list of retrieved Returns","title":"Classification"},{"location":"handlers/metrics/classification/#classification95metrics","text":"def classification_metrics ( ground_truth , retrieved ) Given ground truth dictionary and retrieved dictionary, return per class precision, recall and f1 score. Class 1 is assigned to duplicate file pairs while class 0 is for non-duplicate file pairs.","title":"classification_metrics"},{"location":"handlers/metrics/classification/#args","text":"ground_truth : A dictionary representing ground truth with filenames as key and a list of duplicate filenames retrieved : A dictionary representing retrieved duplicates with filenames as key and a list of retrieved","title":"Args"},{"location":"handlers/metrics/classification/#returns","text":"","title":"Returns"},{"location":"handlers/metrics/information_retrieval/","text":"avg_prec def avg_prec ( correct_duplicates , retrieved_duplicates ) Get average precision(AP) for a single query given correct and retrieved file names. Args correct_duplicates : List of correct duplicates i.e., ground truth) retrieved_duplicates : List of retrieved duplicates for one single query Returns ndcg def ndcg ( correct_duplicates , retrieved_duplicates ) Get Normalized discounted cumulative gain(NDCG) for a single query given correct and retrieved file names. Args correct_duplicates : List of correct duplicates i.e., ground truth) retrieved_duplicates : List of retrieved duplicates for one single query Returns jaccard_similarity def jaccard_similarity ( correct_duplicates , retrieved_duplicates ) Get jaccard similarity for a single query given correct and retrieved file names. Args correct_duplicates : List of correct duplicates i.e., ground truth) retrieved_duplicates : List of retrieved duplicates for one single query Returns mean_metric def mean_metric ( ground_truth , retrieved , metric ) Get mean of specified metric. Args metric_func : metric function on which mean is to be calculated across all queries Returns get_all_metrics def get_all_metrics ( ground_truth , retrieved ) Get mean of all information retrieval metrics across all queries. Args ground_truth : A dictionary representing ground truth with filenames as key and a list of duplicate filenames retrieved : A dictionary representing retrieved duplicates with filenames as key and a list of retrieved Returns","title":"Information retrieval"},{"location":"handlers/metrics/information_retrieval/#avg95prec","text":"def avg_prec ( correct_duplicates , retrieved_duplicates ) Get average precision(AP) for a single query given correct and retrieved file names.","title":"avg_prec"},{"location":"handlers/metrics/information_retrieval/#args","text":"correct_duplicates : List of correct duplicates i.e., ground truth) retrieved_duplicates : List of retrieved duplicates for one single query","title":"Args"},{"location":"handlers/metrics/information_retrieval/#returns","text":"","title":"Returns"},{"location":"handlers/metrics/information_retrieval/#ndcg","text":"def ndcg ( correct_duplicates , retrieved_duplicates ) Get Normalized discounted cumulative gain(NDCG) for a single query given correct and retrieved file names.","title":"ndcg"},{"location":"handlers/metrics/information_retrieval/#args_1","text":"correct_duplicates : List of correct duplicates i.e., ground truth) retrieved_duplicates : List of retrieved duplicates for one single query","title":"Args"},{"location":"handlers/metrics/information_retrieval/#returns_1","text":"","title":"Returns"},{"location":"handlers/metrics/information_retrieval/#jaccard95similarity","text":"def jaccard_similarity ( correct_duplicates , retrieved_duplicates ) Get jaccard similarity for a single query given correct and retrieved file names.","title":"jaccard_similarity"},{"location":"handlers/metrics/information_retrieval/#args_2","text":"correct_duplicates : List of correct duplicates i.e., ground truth) retrieved_duplicates : List of retrieved duplicates for one single query","title":"Args"},{"location":"handlers/metrics/information_retrieval/#returns_2","text":"","title":"Returns"},{"location":"handlers/metrics/information_retrieval/#mean95metric","text":"def mean_metric ( ground_truth , retrieved , metric ) Get mean of specified metric.","title":"mean_metric"},{"location":"handlers/metrics/information_retrieval/#args_3","text":"metric_func : metric function on which mean is to be calculated across all queries","title":"Args"},{"location":"handlers/metrics/information_retrieval/#returns_3","text":"","title":"Returns"},{"location":"handlers/metrics/information_retrieval/#get95all95metrics","text":"def get_all_metrics ( ground_truth , retrieved ) Get mean of all information retrieval metrics across all queries.","title":"get_all_metrics"},{"location":"handlers/metrics/information_retrieval/#args_4","text":"ground_truth : A dictionary representing ground truth with filenames as key and a list of duplicate filenames retrieved : A dictionary representing retrieved duplicates with filenames as key and a list of retrieved","title":"Args"},{"location":"handlers/metrics/information_retrieval/#returns_4","text":"","title":"Returns"},{"location":"handlers/search/bktree/","text":"class BkTreeNode Class to contain the attributes of a single node in the BKTree. __init__ def __init__ ( node_name , node_value , parent_name ) class BKTree Class to construct and perform search using a BKTree. __init__ def __init__ ( hash_dict , distance_function ) Initialize a root for the BKTree and triggers the tree construction using the dictionary for mapping file names and corresponding hashes. Args hash_dict : Dictionary mapping file names to corresponding hash strings {filename: hash} distance_function : A function for calculating distance between the hashes. construct_tree def construct_tree () Construct the BKTree. search def search ( query , tol ) Function to search the bktree given a hash of the query image. Args query : hash string for which BKTree needs to be searched. tol : distance upto which duplicate is valid. Returns List of tuples of the form [(valid_retrieval_filename1 : distance), (valid_retrieval_filename2: distance)]","title":"Bktree"},{"location":"handlers/search/bktree/#class-bktreenode","text":"Class to contain the attributes of a single node in the BKTree.","title":"class BkTreeNode"},{"location":"handlers/search/bktree/#9595init9595","text":"def __init__ ( node_name , node_value , parent_name )","title":"__init__"},{"location":"handlers/search/bktree/#class-bktree","text":"Class to construct and perform search using a BKTree.","title":"class BKTree"},{"location":"handlers/search/bktree/#9595init9595_1","text":"def __init__ ( hash_dict , distance_function ) Initialize a root for the BKTree and triggers the tree construction using the dictionary for mapping file names and corresponding hashes.","title":"__init__"},{"location":"handlers/search/bktree/#args","text":"hash_dict : Dictionary mapping file names to corresponding hash strings {filename: hash} distance_function : A function for calculating distance between the hashes.","title":"Args"},{"location":"handlers/search/bktree/#construct95tree","text":"def construct_tree () Construct the BKTree.","title":"construct_tree"},{"location":"handlers/search/bktree/#search","text":"def search ( query , tol ) Function to search the bktree given a hash of the query image.","title":"search"},{"location":"handlers/search/bktree/#args_1","text":"query : hash string for which BKTree needs to be searched. tol : distance upto which duplicate is valid.","title":"Args"},{"location":"handlers/search/bktree/#returns","text":"List of tuples of the form [(valid_retrieval_filename1 : distance), (valid_retrieval_filename2: distance)]","title":"Returns"},{"location":"handlers/search/brute_force/","text":"class BruteForce Class to perform search using a Brute force. __init__ def __init__ ( hash_dict , distance_function ) Initialize a dictionary for mapping file names and corresponding hashes anda distance function to be used for getting distance between two hash strings. Args hash_dict : Dictionary mapping file names to corresponding hash strings {filename: hash} distance_function : A function for calculating distance between the hashes. search def search ( query , tol ) Function for searching using brute force. Args query : hash string for which brute force needs to work. tol : distance upto which duplicate is valid. Returns List of tuples of the form [(valid_retrieval_filename1 : distance), (valid_retrieval_filename2: distance)]","title":"Brute force"},{"location":"handlers/search/brute_force/#class-bruteforce","text":"Class to perform search using a Brute force.","title":"class BruteForce"},{"location":"handlers/search/brute_force/#9595init9595","text":"def __init__ ( hash_dict , distance_function ) Initialize a dictionary for mapping file names and corresponding hashes anda distance function to be used for getting distance between two hash strings.","title":"__init__"},{"location":"handlers/search/brute_force/#args","text":"hash_dict : Dictionary mapping file names to corresponding hash strings {filename: hash} distance_function : A function for calculating distance between the hashes.","title":"Args"},{"location":"handlers/search/brute_force/#search","text":"def search ( query , tol ) Function for searching using brute force.","title":"search"},{"location":"handlers/search/brute_force/#args_1","text":"query : hash string for which brute force needs to work. tol : distance upto which duplicate is valid.","title":"Args"},{"location":"handlers/search/brute_force/#returns","text":"List of tuples of the form [(valid_retrieval_filename1 : distance), (valid_retrieval_filename2: distance)]","title":"Returns"},{"location":"handlers/search/retrieval/","text":"class HashEval __init__ def __init__ ( test , queries , distance_function , threshold , search_method ) Initialize a HashEval object which offers an interface to control hashing and search methods for desired dataset. Compute a map of duplicate images in the document space given certain input control parameters. retrieve_results def retrieve_results ( scores ) Return results with or without scores. Args scores : Boolean indicating whether results are to eb returned with or without scores. Returns if scores is True, then a dictionary of the form {'image1.jpg' : [('image1_duplicate1.jpg', score), ('image1_duplicate2.jpg', score)], 'image2.jpg' : [] ..} if scores is False, then a dictionary of the form {'image1.jpg' : ['image1_duplicate1.jpg', 'image1_duplicate2.jpg'], 'image2.jpg' : ['image1_duplicate1.jpg',..], ..}","title":"Retrieval"},{"location":"handlers/search/retrieval/#class-hasheval","text":"","title":"class HashEval"},{"location":"handlers/search/retrieval/#9595init9595","text":"def __init__ ( test , queries , distance_function , threshold , search_method ) Initialize a HashEval object which offers an interface to control hashing and search methods for desired dataset. Compute a map of duplicate images in the document space given certain input control parameters.","title":"__init__"},{"location":"handlers/search/retrieval/#retrieve95results","text":"def retrieve_results ( scores ) Return results with or without scores.","title":"retrieve_results"},{"location":"handlers/search/retrieval/#args","text":"scores : Boolean indicating whether results are to eb returned with or without scores.","title":"Args"},{"location":"handlers/search/retrieval/#returns","text":"if scores is True, then a dictionary of the form {'image1.jpg' : [('image1_duplicate1.jpg', score), ('image1_duplicate2.jpg', score)], 'image2.jpg' : [] ..} if scores is False, then a dictionary of the form {'image1.jpg' : ['image1_duplicate1.jpg', 'image1_duplicate2.jpg'], 'image2.jpg' : ['image1_duplicate1.jpg',..], ..}","title":"Returns"},{"location":"methods/cnn/","text":"class CNN Find duplicates using CNN and/or generate CNN features given a single image or a directory of images. The module can be used for 2 purposes: Feature generation and duplicate detection. Feature generation: To propagate an image through a Convolutional Neural Network architecture and generate features. The generated features can be used at a later time for deduplication. Using the method 'encode_image', the CNN feature for a single image can be obtained while the 'encode_images' method can be used to get features for all images in a directory. Duplicate detection: Find duplicates either using the feature mapping generated previously using 'encode_images' or using a Path to the directory that contains the images that need to be deduplicated. 'find_duplciates' and 'find_duplicates_to_remove' methods are provided to accomplish these tasks. __init__ def __init__ () Initialize a keras MobileNet model that is sliced at the last convolutional layer. Set the batch size for keras generators to be 64 samples. Set the input image size to (224, 224) for providing as input to MobileNet model. encode_image def encode_image ( image_file , image_array ) Generate CNN features for a single image. Args image_file : Path to the image file. image_array : Image typecast to numpy array. Returns feature : Features for the image in the form of numpy array. Example usage from imagededup.methods import CNN myencoder = CNN () feature_vector = myencoder . encode_image ( image_file = 'path/to/image.jpg' ) OR feature_vector = myencoder . encode_image ( image_array =< numpy array of image > ) encode_images def encode_images ( image_dir ) Generate CNN features for all images in a given directory of images. Args image_dir : Path to the image directory. Returns dictionary : Contains a mapping of filenames and corresponding numpy array of CNN features. Example usage from imagededup.methods import CNN myencoder = CNN () feature_map = myencoder . encode_images ( image_dir = 'path/to/image/directory' ) find_duplicates def find_duplicates ( image_dir , encoding_map , min_similarity_threshold , scores , outfile ) Find duplicates for each file. Take in path of the directory or encoding dictionary in which duplicates are to be detected above the given threshold. Return dictionary containing key as filename and value as a list of duplicate file names. Optionally, the cosine distances could be returned instead of just duplicate filenames for each query file. Args image_dir : Path to the directory containing all the images or dictionary with keys as file names encoding_map : A dictionary containing mapping of filenames and corresponding CNN features. min_similarity_threshold : Threshold value (must be float between -1.0 and 1.0). Default is 0.9 scores : Boolean indicating whether similarity scores are to be returned along with retrieved duplicates. outfile : Name of the file to save the results. Returns dictionary : if scores is True, then a dictionary of the form {'image1.jpg': [('image1_duplicate1.jpg', score), ('image1_duplicate2.jpg', score)], 'image2.jpg': [] ..}. if scores is False, then a dictionary of the form {'image1.jpg': ['image1_duplicate1.jpg', 'image1_duplicate2.jpg'], 'image2.jpg':['image1_duplicate1.jpg',..], ..} Example usage from imagededup.methods import CNN myencoder = CNN () duplicates = myencoder . find_duplicates ( image_dir = 'path/to/directory' , min_similarity_threshold = 15 , scores = True , outfile = 'results.json' ) OR from imagededup.methods import CNN myencoder = CNN () duplicates = myencoder . find_duplicates ( encoding_map =< mapping filename to cnn features > , min_similarity_threshold = 15 , scores = True , outfile = 'results.json' ) find_duplicates_to_remove def find_duplicates_to_remove ( image_dir , encoding_map , min_similarity_threshold , outfile ) Give out a list of image file names to remove based on the similarity threshold. Does not remove the mentioned files. Args image_dir : Path to the directory containing all the images or dictionary with keys as file names and values as numpy arrays which represent the CNN feature for the key image file. encoding_map : A dictionary containing mapping of filenames and corresponding CNN features. min_similarity_threshold : Threshold value (must be float between -1.0 and 1.0). Default is 0.9 outfile : Name of the file to save the results. Returns duplicates : List of image file names that should be removed. Example usage from imagededup.methods import CNN myencoder = CNN () duplicates = myencoder . find_duplicates_to_remove ( image_dir = 'path/to/images/directory' ), min_similarity_threshold = 15 ) OR from imagededup.methods import CNN myencoder = CNN () duplicates = myencoder . find_duplicates_to_remove ( encoding_map =< mapping filename to cnn features > , min_similarity_threshold = 15 , outfile = 'results.json' )","title":"CNN"},{"location":"methods/cnn/#class-cnn","text":"Find duplicates using CNN and/or generate CNN features given a single image or a directory of images. The module can be used for 2 purposes: Feature generation and duplicate detection. Feature generation: To propagate an image through a Convolutional Neural Network architecture and generate features. The generated features can be used at a later time for deduplication. Using the method 'encode_image', the CNN feature for a single image can be obtained while the 'encode_images' method can be used to get features for all images in a directory. Duplicate detection: Find duplicates either using the feature mapping generated previously using 'encode_images' or using a Path to the directory that contains the images that need to be deduplicated. 'find_duplciates' and 'find_duplicates_to_remove' methods are provided to accomplish these tasks.","title":"class CNN"},{"location":"methods/cnn/#9595init9595","text":"def __init__ () Initialize a keras MobileNet model that is sliced at the last convolutional layer. Set the batch size for keras generators to be 64 samples. Set the input image size to (224, 224) for providing as input to MobileNet model.","title":"__init__"},{"location":"methods/cnn/#encode95image","text":"def encode_image ( image_file , image_array ) Generate CNN features for a single image.","title":"encode_image"},{"location":"methods/cnn/#args","text":"image_file : Path to the image file. image_array : Image typecast to numpy array.","title":"Args"},{"location":"methods/cnn/#returns","text":"feature : Features for the image in the form of numpy array.","title":"Returns"},{"location":"methods/cnn/#example-usage","text":"from imagededup.methods import CNN myencoder = CNN () feature_vector = myencoder . encode_image ( image_file = 'path/to/image.jpg' ) OR feature_vector = myencoder . encode_image ( image_array =< numpy array of image > )","title":"Example usage"},{"location":"methods/cnn/#encode95images","text":"def encode_images ( image_dir ) Generate CNN features for all images in a given directory of images.","title":"encode_images"},{"location":"methods/cnn/#args_1","text":"image_dir : Path to the image directory.","title":"Args"},{"location":"methods/cnn/#returns_1","text":"dictionary : Contains a mapping of filenames and corresponding numpy array of CNN features.","title":"Returns"},{"location":"methods/cnn/#example-usage_1","text":"from imagededup.methods import CNN myencoder = CNN () feature_map = myencoder . encode_images ( image_dir = 'path/to/image/directory' )","title":"Example usage"},{"location":"methods/cnn/#find95duplicates","text":"def find_duplicates ( image_dir , encoding_map , min_similarity_threshold , scores , outfile ) Find duplicates for each file. Take in path of the directory or encoding dictionary in which duplicates are to be detected above the given threshold. Return dictionary containing key as filename and value as a list of duplicate file names. Optionally, the cosine distances could be returned instead of just duplicate filenames for each query file.","title":"find_duplicates"},{"location":"methods/cnn/#args_2","text":"image_dir : Path to the directory containing all the images or dictionary with keys as file names encoding_map : A dictionary containing mapping of filenames and corresponding CNN features. min_similarity_threshold : Threshold value (must be float between -1.0 and 1.0). Default is 0.9 scores : Boolean indicating whether similarity scores are to be returned along with retrieved duplicates. outfile : Name of the file to save the results.","title":"Args"},{"location":"methods/cnn/#returns_2","text":"dictionary : if scores is True, then a dictionary of the form {'image1.jpg': [('image1_duplicate1.jpg', score), ('image1_duplicate2.jpg', score)], 'image2.jpg': [] ..}. if scores is False, then a dictionary of the form {'image1.jpg': ['image1_duplicate1.jpg', 'image1_duplicate2.jpg'], 'image2.jpg':['image1_duplicate1.jpg',..], ..}","title":"Returns"},{"location":"methods/cnn/#example-usage_2","text":"from imagededup.methods import CNN myencoder = CNN () duplicates = myencoder . find_duplicates ( image_dir = 'path/to/directory' , min_similarity_threshold = 15 , scores = True , outfile = 'results.json' ) OR from imagededup.methods import CNN myencoder = CNN () duplicates = myencoder . find_duplicates ( encoding_map =< mapping filename to cnn features > , min_similarity_threshold = 15 , scores = True , outfile = 'results.json' )","title":"Example usage"},{"location":"methods/cnn/#find95duplicates95to95remove","text":"def find_duplicates_to_remove ( image_dir , encoding_map , min_similarity_threshold , outfile ) Give out a list of image file names to remove based on the similarity threshold. Does not remove the mentioned files.","title":"find_duplicates_to_remove"},{"location":"methods/cnn/#args_3","text":"image_dir : Path to the directory containing all the images or dictionary with keys as file names and values as numpy arrays which represent the CNN feature for the key image file. encoding_map : A dictionary containing mapping of filenames and corresponding CNN features. min_similarity_threshold : Threshold value (must be float between -1.0 and 1.0). Default is 0.9 outfile : Name of the file to save the results.","title":"Args"},{"location":"methods/cnn/#returns_3","text":"duplicates : List of image file names that should be removed.","title":"Returns"},{"location":"methods/cnn/#example-usage_3","text":"from imagededup.methods import CNN myencoder = CNN () duplicates = myencoder . find_duplicates_to_remove ( image_dir = 'path/to/images/directory' ), min_similarity_threshold = 15 ) OR from imagededup.methods import CNN myencoder = CNN () duplicates = myencoder . find_duplicates_to_remove ( encoding_map =< mapping filename to cnn features > , min_similarity_threshold = 15 , outfile = 'results.json' )","title":"Example usage"},{"location":"methods/hashing/","text":"class Hashing Find duplicates using hashing algorithms and/or generate hashes given a single image or a directory of images. The module can be used for 2 purposes: Feature generation and duplicate detection. Feature generation: To generate hashes using specific hashing method. The generated hashes can be used at a later time for deduplication. Using the method 'encode_image' from the specific hashing method object, the hash for a single image can be obtained while the 'encode_images' method can be used to get hashes for all images in a directory. Duplicate detection: Find duplicates either using the feature mapping generated previously using 'encode_images' or using a Path to the directory that contains the images that need to be deduplicated. 'find_duplciates' and 'find_duplicates_to_remove' methods are provided to accomplish these tasks. __init__ def __init__ () hamming_distance def hamming_distance ( hash1 , hash2 ) Calculate the hamming distance between two hashes. If length of hashes is not 64 bits, then pads the length to be 64 for each hash and then calculates the hamming distance. Args hash1 : hash string hash2 : hash string Returns hamming_distance : Hamming distance between the two hashes. encode_image def encode_image ( image_file , image_array ) Generate hash for a single image. Args image_file : Path to the image file. image_array : Image typecast to numpy array. Returns hash : A 16 character hexadecimal string hash for the image. Example usage from imagededup.methods import < hash - method > myencoder = < hash - method > () myhash = myencoder . encode_image ( image_file = 'path/to/image.jpg' ) OR myhash = myencoder . encode_image ( image_array =< numpy array of image > ) encode_images def encode_images ( image_dir ) Generate hashes for all images in a given directory of images. Args image_dir : Path to the image directory. Returns dictionary : A dictionary that contains a mapping of filenames and corresponding 64 character hash string such as {'Image1.jpg': 'hash_string1', 'Image2.jpg': 'hash_string2', ...} Example usage from imagededup.methods import < hash - method > myencoder = < hash - method > () mapping = myencoder . encode_images ( 'path/to/directory' ) find_duplicates def find_duplicates ( image_dir , encoding_map , max_distance_threshold , scores , outfile ) Find duplicates for each file. Takes in path of the directory or encoding dictionary in which duplicates are to be detected. All images with hamming distance less than or equal to the max_distance_threshold are regarded as duplicates. Returns dictionary containing key as filename and value as a list of duplicate file names. Optionally, the below the given hamming distance could be returned instead of just duplicate filenames for each query file. Args image_dir : Path to the directory containing all the images or dictionary with keys as file names and values as hash strings for the key image file. encoding_map : A dictionary containing mapping of filenames and corresponding hashes. max_distance_threshold : Hamming distance between two images below which retrieved duplicates are valid. (must be an int between 0 and 64). Default is 10. scores : Boolean indicating whether Hamming distances are to be returned along with retrieved duplicates. outfile : Name of the file to save the results. Returns duplicates dictionary : if scores is True, then a dictionary of the form {'image1.jpg': [('image1_duplicate1.jpg', score), ('image1_duplicate2.jpg', score)], 'image2.jpg': [] ..}. if scores is False, then a dictionary of the form {'image1.jpg': ['image1_duplicate1.jpg', 'image1_duplicate2.jpg'], 'image2.jpg':['image1_duplicate1.jpg',..], ..} Example usage from imagededup.methods import < hash - method > myencoder = < hash - method > () duplicates = myencoder . find_duplicates ( image_dir = 'path/to/directory' , max_distance_threshold = 15 , scores = True , outfile = 'results.json' ) OR from imagededup.methods import < hash - method > myencoder = < hash - method > () duplicates = myencoder . find_duplicates ( encoding_map =< mapping filename to hashes > , max_distance_threshold = 15 , scores = True , outfile = 'results.json' ) find_duplicates_to_remove def find_duplicates_to_remove ( image_dir , encoding_map , max_distance_threshold , outfile ) Give out a list of image file names to remove based on the hamming distance threshold threshold. Does not remove the mentioned files. Args image_dir : Path to the directory containing all the images or dictionary with keys as file names and values as hash strings for the key image file. encoding_map : A dictionary containing mapping of filenames and corresponding hashes. max_distance_threshold : Hamming distance between two images below which retrieved duplicates are valid. (must be an int between 0 and 64). Default is 10. outfile : Name of the file to save the results. Returns duplicates : List of image file names that are found to be duplicate of me other file in the directory. Example usage from imagededup.methods import < hash - method > myencoder = < hash - method > () duplicates = myencoder . find_duplicates_to_remove ( image_dir = 'path/to/images/directory' ), max_distance_threshold = 15 ) OR from imagededup.methods import < hash - method > myencoder = < hash - method > () duplicates = myencoder . find_duplicates ( encoding_map =< mapping filename to hashes > , max_distance_threshold = 15 , outfile = 'results.json' ) class PHash Inherits from Hashing base class and implements perceptual hashing (Implementation reference: http://www.hackerfactor.com/blog/index.php?/archives/432-Looks-Like-It.html). Offers all the functionality mentioned in hashing class. Example usage # Perceptual hash for images from imagededup.methods import PHash phasher = PHash () perceptual_hash = phasher . encode_image ( image_file = 'path/to/image.jpg' ) OR perceptual_hash = phasher . encode_image ( image_array = < numpy image array > ) OR perceptual_hashes = phasher . encode_images ( image_dir = 'path/to/directory' ) # for a directory of images # Finding duplicates: from imagededup.methods import PHash phasher = PHash () duplicates = phasher . find_duplicates ( image_dir = 'path/to/directory' , max_distance_threshold = 15 , scores = True ) OR duplicates = phasher . find_duplicates ( encoding_map = encoding_map , max_distance_threshold = 15 , scores = True ) # Finding duplicates to return a single list of duplicates in the image collection from imagededup.methods import PHash phasher = PHash () files_to_remove = phasher . find_duplicates_to_remove ( image_dir = 'path/to/images/directory' , max_distance_threshold = 15 ) OR files_to_remove = phasher . find_duplicates_to_remove ( encoding_map = encoding_map , max_distance_threshold = 15 ) __init__ def __init__ () class AHash Inherits from Hashing base class and implements average hashing. (Implementation reference: http://www.hackerfactor.com/blog/index.php?/archives/529-Kind-of-Like-That.html) Offers all the functionality mentioned in hashing class. Example usage # Average hash for images from imagededup.methods import AHash ahasher = AHash () average_hash = ahasher . encode_image ( image_file = 'path/to/image.jpg' ) OR average_hash = ahasher . encode_image ( image_array = < numpy image array > ) OR average_hashes = ahasher . encode_images ( image_dir = 'path/to/directory' ) # for a directory of images # Finding duplicates: from imagededup.methods import AHash ahasher = AHash () duplicates = ahasher . find_duplicates ( image_dir = 'path/to/directory' , max_distance_threshold = 15 , scores = True ) OR duplicates = ahasher . find_duplicates ( encoding_map = encoding_map , max_distance_threshold = 15 , scores = True ) # Finding duplicates to return a single list of duplicates in the image collection from imagededup.methods import AHash ahasher = AHash () files_to_remove = ahasher . find_duplicates_to_remove ( image_dir = 'path/to/images/directory' , max_distance_threshold = 15 ) OR files_to_remove = ahasher . find_duplicates_to_remove ( encoding_map = encoding_map , max_distance_threshold = 15 ) __init__ def __init__ () class DHash Inherits from Hashing base class and implements difference hashing. (Implementation reference: http://www.hackerfactor.com/blog/index.php?/archives/529-Kind-of-Like-That.html) Offers all the functionality mentioned in hashing class. Example usage # Difference hash for images from imagededup.methods import DHash dhasher = DHash () difference_hash = dhasher . encode_image ( image_file = 'path/to/image.jpg' ) OR difference_hash = dhasher . encode_image ( image_array = < numpy image array > ) OR difference_hashes = dhasher . encode_images ( image_dir = 'path/to/directory' ) # for a directory of images # Finding duplicates: from imagededup.methods import DHash dhasher = DHash () duplicates = dhasher . find_duplicates ( image_dir = 'path/to/directory' , max_distance_threshold = 15 , scores = True ) OR duplicates = dhasher . find_duplicates ( encoding_map = encoding_map , max_distance_threshold = 15 , scores = True ) # Finding duplicates to return a single list of duplicates in the image collection from imagededup.methods import DHash dhasher = DHash () files_to_remove = dhasher . find_duplicates_to_remove ( image_dir = 'path/to/images/directory' , max_distance_threshold = 15 ) OR files_to_remove = dhasher . find_duplicates_to_remove ( encoding_map = encoding_map , max_distance_threshold = 15 ) __init__ def __init__ () class WHash Inherits from Hashing base class and implements wavelet hashing. (Implementation reference: https://fullstackml.com/wavelet-image-hash-in-python-3504fdd282b5) Offers all the functionality mentioned in hashing class. Example usage # Wavelet hash for images from imagededup.methods import WHash whasher = WHash () wavelet_hash = whasher . encode_image ( image_file = 'path/to/image.jpg' ) OR wavelet_hash = whasher . encode_image ( image_array = < numpy image array > ) OR wavelet_hashes = whasher . encode_images ( image_dir = 'path/to/directory' ) # for a directory of images # Finding duplicates: from imagededup.methods import WHash whasher = WHash () duplicates = whasher . find_duplicates ( image_dir = 'path/to/directory' , max_distance_threshold = 15 , scores = True ) OR duplicates = whasher . find_duplicates ( encoding_map = encoding_map , max_distance_threshold = 15 , scores = True ) # Finding duplicates to return a single list of duplicates in the image collection from imagededup.methods import WHash whasher = WHash () files_to_remove = whasher . find_duplicates_to_remove ( image_dir = 'path/to/images/directory' , max_distance_threshold = 15 ) OR files_to_remove = whasher . find_duplicates_to_remove ( encoding_map = encoding_map , max_distance_threshold = 15 ) __init__ def __init__ ()","title":"Hashing"},{"location":"methods/hashing/#class-hashing","text":"Find duplicates using hashing algorithms and/or generate hashes given a single image or a directory of images. The module can be used for 2 purposes: Feature generation and duplicate detection. Feature generation: To generate hashes using specific hashing method. The generated hashes can be used at a later time for deduplication. Using the method 'encode_image' from the specific hashing method object, the hash for a single image can be obtained while the 'encode_images' method can be used to get hashes for all images in a directory. Duplicate detection: Find duplicates either using the feature mapping generated previously using 'encode_images' or using a Path to the directory that contains the images that need to be deduplicated. 'find_duplciates' and 'find_duplicates_to_remove' methods are provided to accomplish these tasks.","title":"class Hashing"},{"location":"methods/hashing/#9595init9595","text":"def __init__ ()","title":"__init__"},{"location":"methods/hashing/#hamming95distance","text":"def hamming_distance ( hash1 , hash2 ) Calculate the hamming distance between two hashes. If length of hashes is not 64 bits, then pads the length to be 64 for each hash and then calculates the hamming distance.","title":"hamming_distance"},{"location":"methods/hashing/#args","text":"hash1 : hash string hash2 : hash string","title":"Args"},{"location":"methods/hashing/#returns","text":"hamming_distance : Hamming distance between the two hashes.","title":"Returns"},{"location":"methods/hashing/#encode95image","text":"def encode_image ( image_file , image_array ) Generate hash for a single image.","title":"encode_image"},{"location":"methods/hashing/#args_1","text":"image_file : Path to the image file. image_array : Image typecast to numpy array.","title":"Args"},{"location":"methods/hashing/#returns_1","text":"hash : A 16 character hexadecimal string hash for the image.","title":"Returns"},{"location":"methods/hashing/#example-usage","text":"from imagededup.methods import < hash - method > myencoder = < hash - method > () myhash = myencoder . encode_image ( image_file = 'path/to/image.jpg' ) OR myhash = myencoder . encode_image ( image_array =< numpy array of image > )","title":"Example usage"},{"location":"methods/hashing/#encode95images","text":"def encode_images ( image_dir ) Generate hashes for all images in a given directory of images.","title":"encode_images"},{"location":"methods/hashing/#args_2","text":"image_dir : Path to the image directory.","title":"Args"},{"location":"methods/hashing/#returns_2","text":"dictionary : A dictionary that contains a mapping of filenames and corresponding 64 character hash string such as {'Image1.jpg': 'hash_string1', 'Image2.jpg': 'hash_string2', ...}","title":"Returns"},{"location":"methods/hashing/#example-usage_1","text":"from imagededup.methods import < hash - method > myencoder = < hash - method > () mapping = myencoder . encode_images ( 'path/to/directory' )","title":"Example usage"},{"location":"methods/hashing/#find95duplicates","text":"def find_duplicates ( image_dir , encoding_map , max_distance_threshold , scores , outfile ) Find duplicates for each file. Takes in path of the directory or encoding dictionary in which duplicates are to be detected. All images with hamming distance less than or equal to the max_distance_threshold are regarded as duplicates. Returns dictionary containing key as filename and value as a list of duplicate file names. Optionally, the below the given hamming distance could be returned instead of just duplicate filenames for each query file.","title":"find_duplicates"},{"location":"methods/hashing/#args_3","text":"image_dir : Path to the directory containing all the images or dictionary with keys as file names and values as hash strings for the key image file. encoding_map : A dictionary containing mapping of filenames and corresponding hashes. max_distance_threshold : Hamming distance between two images below which retrieved duplicates are valid. (must be an int between 0 and 64). Default is 10. scores : Boolean indicating whether Hamming distances are to be returned along with retrieved duplicates. outfile : Name of the file to save the results.","title":"Args"},{"location":"methods/hashing/#returns_3","text":"duplicates dictionary : if scores is True, then a dictionary of the form {'image1.jpg': [('image1_duplicate1.jpg', score), ('image1_duplicate2.jpg', score)], 'image2.jpg': [] ..}. if scores is False, then a dictionary of the form {'image1.jpg': ['image1_duplicate1.jpg', 'image1_duplicate2.jpg'], 'image2.jpg':['image1_duplicate1.jpg',..], ..}","title":"Returns"},{"location":"methods/hashing/#example-usage_2","text":"from imagededup.methods import < hash - method > myencoder = < hash - method > () duplicates = myencoder . find_duplicates ( image_dir = 'path/to/directory' , max_distance_threshold = 15 , scores = True , outfile = 'results.json' ) OR from imagededup.methods import < hash - method > myencoder = < hash - method > () duplicates = myencoder . find_duplicates ( encoding_map =< mapping filename to hashes > , max_distance_threshold = 15 , scores = True , outfile = 'results.json' )","title":"Example usage"},{"location":"methods/hashing/#find95duplicates95to95remove","text":"def find_duplicates_to_remove ( image_dir , encoding_map , max_distance_threshold , outfile ) Give out a list of image file names to remove based on the hamming distance threshold threshold. Does not remove the mentioned files.","title":"find_duplicates_to_remove"},{"location":"methods/hashing/#args_4","text":"image_dir : Path to the directory containing all the images or dictionary with keys as file names and values as hash strings for the key image file. encoding_map : A dictionary containing mapping of filenames and corresponding hashes. max_distance_threshold : Hamming distance between two images below which retrieved duplicates are valid. (must be an int between 0 and 64). Default is 10. outfile : Name of the file to save the results.","title":"Args"},{"location":"methods/hashing/#returns_4","text":"duplicates : List of image file names that are found to be duplicate of me other file in the directory.","title":"Returns"},{"location":"methods/hashing/#example-usage_3","text":"from imagededup.methods import < hash - method > myencoder = < hash - method > () duplicates = myencoder . find_duplicates_to_remove ( image_dir = 'path/to/images/directory' ), max_distance_threshold = 15 ) OR from imagededup.methods import < hash - method > myencoder = < hash - method > () duplicates = myencoder . find_duplicates ( encoding_map =< mapping filename to hashes > , max_distance_threshold = 15 , outfile = 'results.json' )","title":"Example usage"},{"location":"methods/hashing/#class-phash","text":"Inherits from Hashing base class and implements perceptual hashing (Implementation reference: http://www.hackerfactor.com/blog/index.php?/archives/432-Looks-Like-It.html). Offers all the functionality mentioned in hashing class.","title":"class PHash"},{"location":"methods/hashing/#example-usage_4","text":"# Perceptual hash for images from imagededup.methods import PHash phasher = PHash () perceptual_hash = phasher . encode_image ( image_file = 'path/to/image.jpg' ) OR perceptual_hash = phasher . encode_image ( image_array = < numpy image array > ) OR perceptual_hashes = phasher . encode_images ( image_dir = 'path/to/directory' ) # for a directory of images # Finding duplicates: from imagededup.methods import PHash phasher = PHash () duplicates = phasher . find_duplicates ( image_dir = 'path/to/directory' , max_distance_threshold = 15 , scores = True ) OR duplicates = phasher . find_duplicates ( encoding_map = encoding_map , max_distance_threshold = 15 , scores = True ) # Finding duplicates to return a single list of duplicates in the image collection from imagededup.methods import PHash phasher = PHash () files_to_remove = phasher . find_duplicates_to_remove ( image_dir = 'path/to/images/directory' , max_distance_threshold = 15 ) OR files_to_remove = phasher . find_duplicates_to_remove ( encoding_map = encoding_map , max_distance_threshold = 15 )","title":"Example usage"},{"location":"methods/hashing/#9595init9595_1","text":"def __init__ ()","title":"__init__"},{"location":"methods/hashing/#class-ahash","text":"Inherits from Hashing base class and implements average hashing. (Implementation reference: http://www.hackerfactor.com/blog/index.php?/archives/529-Kind-of-Like-That.html) Offers all the functionality mentioned in hashing class.","title":"class AHash"},{"location":"methods/hashing/#example-usage_5","text":"# Average hash for images from imagededup.methods import AHash ahasher = AHash () average_hash = ahasher . encode_image ( image_file = 'path/to/image.jpg' ) OR average_hash = ahasher . encode_image ( image_array = < numpy image array > ) OR average_hashes = ahasher . encode_images ( image_dir = 'path/to/directory' ) # for a directory of images # Finding duplicates: from imagededup.methods import AHash ahasher = AHash () duplicates = ahasher . find_duplicates ( image_dir = 'path/to/directory' , max_distance_threshold = 15 , scores = True ) OR duplicates = ahasher . find_duplicates ( encoding_map = encoding_map , max_distance_threshold = 15 , scores = True ) # Finding duplicates to return a single list of duplicates in the image collection from imagededup.methods import AHash ahasher = AHash () files_to_remove = ahasher . find_duplicates_to_remove ( image_dir = 'path/to/images/directory' , max_distance_threshold = 15 ) OR files_to_remove = ahasher . find_duplicates_to_remove ( encoding_map = encoding_map , max_distance_threshold = 15 )","title":"Example usage"},{"location":"methods/hashing/#9595init9595_2","text":"def __init__ ()","title":"__init__"},{"location":"methods/hashing/#class-dhash","text":"Inherits from Hashing base class and implements difference hashing. (Implementation reference: http://www.hackerfactor.com/blog/index.php?/archives/529-Kind-of-Like-That.html) Offers all the functionality mentioned in hashing class.","title":"class DHash"},{"location":"methods/hashing/#example-usage_6","text":"# Difference hash for images from imagededup.methods import DHash dhasher = DHash () difference_hash = dhasher . encode_image ( image_file = 'path/to/image.jpg' ) OR difference_hash = dhasher . encode_image ( image_array = < numpy image array > ) OR difference_hashes = dhasher . encode_images ( image_dir = 'path/to/directory' ) # for a directory of images # Finding duplicates: from imagededup.methods import DHash dhasher = DHash () duplicates = dhasher . find_duplicates ( image_dir = 'path/to/directory' , max_distance_threshold = 15 , scores = True ) OR duplicates = dhasher . find_duplicates ( encoding_map = encoding_map , max_distance_threshold = 15 , scores = True ) # Finding duplicates to return a single list of duplicates in the image collection from imagededup.methods import DHash dhasher = DHash () files_to_remove = dhasher . find_duplicates_to_remove ( image_dir = 'path/to/images/directory' , max_distance_threshold = 15 ) OR files_to_remove = dhasher . find_duplicates_to_remove ( encoding_map = encoding_map , max_distance_threshold = 15 )","title":"Example usage"},{"location":"methods/hashing/#9595init9595_3","text":"def __init__ ()","title":"__init__"},{"location":"methods/hashing/#class-whash","text":"Inherits from Hashing base class and implements wavelet hashing. (Implementation reference: https://fullstackml.com/wavelet-image-hash-in-python-3504fdd282b5) Offers all the functionality mentioned in hashing class.","title":"class WHash"},{"location":"methods/hashing/#example-usage_7","text":"# Wavelet hash for images from imagededup.methods import WHash whasher = WHash () wavelet_hash = whasher . encode_image ( image_file = 'path/to/image.jpg' ) OR wavelet_hash = whasher . encode_image ( image_array = < numpy image array > ) OR wavelet_hashes = whasher . encode_images ( image_dir = 'path/to/directory' ) # for a directory of images # Finding duplicates: from imagededup.methods import WHash whasher = WHash () duplicates = whasher . find_duplicates ( image_dir = 'path/to/directory' , max_distance_threshold = 15 , scores = True ) OR duplicates = whasher . find_duplicates ( encoding_map = encoding_map , max_distance_threshold = 15 , scores = True ) # Finding duplicates to return a single list of duplicates in the image collection from imagededup.methods import WHash whasher = WHash () files_to_remove = whasher . find_duplicates_to_remove ( image_dir = 'path/to/images/directory' , max_distance_threshold = 15 ) OR files_to_remove = whasher . find_duplicates_to_remove ( encoding_map = encoding_map , max_distance_threshold = 15 )","title":"Example usage"},{"location":"methods/hashing/#9595init9595_4","text":"def __init__ ()","title":"__init__"},{"location":"tutorials/evaluating_performance/","text":"Evaluation of deduplication quality To determine the quality of deduplication algorithm and the corresponding threshold, an evaluation framework is provided. Given a ground truth mapping consisting of file names and a list of duplicates for each file along with a retrieved mapping from the deduplication algorithm for the same files, the following metrics can be obtained using the framework: Mean Average Precision (MAP) Mean Normalized Discounted Cumulative Gain (NDCG) Jaccard Index Per class Precision (class 0 = non-duplicate image pairs, class 1 = duplicate image pairs) Per class Recall (class 0 = non-duplicate image pairs, class 1 = duplicate image pairs) Per class f1-score (class 0 = non-duplicate image pairs, class 1 = duplicate image pairs) The api for obtaining these metrics is as below: from imagededup.evaluation import evaluate metrics = evaluate ( ground_truth_map , retrieved_map , metric = '<metric-name>' ) where the returned variable metrics is a dictionary containing the following content: { 'map' : < map > , 'ndcg' : < mean ndcg > , 'jaccard' : < mean jaccard index > , 'precision' : < numpy array having per class precision > , 'recall' : < numpy array having per class recall > , 'f1-score' : < numpy array having per class f1 - score > , 'support' : < numpy array having per class support > } Options ground_truth_map: A dictionary representing ground truth with filenames as key and a list of duplicate filenames as value. retrieved_map: A dictionary representing retrieved duplicates with filenames as key and a list of retrieved duplicate filenames as value. metric can take one of the following values: 'map' 'ndcg' 'jaccard' 'classification': Returns per class precision, recall, f1-score, support 'all' (default, returns all the above metrics) Considerations Presently, the ground truth map should be prepared manually by the user. Symmetric relations between duplicates must be represented in the ground truth map. If an image i is a duplicate for image j , then j must also be represented as a duplicate of i . Absence of symmetric relations will lead to an exception. Both the ground_truth_map and retrieved_map must have the same keys.","title":"Evaluating performance"},{"location":"tutorials/evaluating_performance/#evaluation-of-deduplication-quality","text":"To determine the quality of deduplication algorithm and the corresponding threshold, an evaluation framework is provided. Given a ground truth mapping consisting of file names and a list of duplicates for each file along with a retrieved mapping from the deduplication algorithm for the same files, the following metrics can be obtained using the framework: Mean Average Precision (MAP) Mean Normalized Discounted Cumulative Gain (NDCG) Jaccard Index Per class Precision (class 0 = non-duplicate image pairs, class 1 = duplicate image pairs) Per class Recall (class 0 = non-duplicate image pairs, class 1 = duplicate image pairs) Per class f1-score (class 0 = non-duplicate image pairs, class 1 = duplicate image pairs) The api for obtaining these metrics is as below: from imagededup.evaluation import evaluate metrics = evaluate ( ground_truth_map , retrieved_map , metric = '<metric-name>' ) where the returned variable metrics is a dictionary containing the following content: { 'map' : < map > , 'ndcg' : < mean ndcg > , 'jaccard' : < mean jaccard index > , 'precision' : < numpy array having per class precision > , 'recall' : < numpy array having per class recall > , 'f1-score' : < numpy array having per class f1 - score > , 'support' : < numpy array having per class support > }","title":"Evaluation of deduplication quality"},{"location":"tutorials/evaluating_performance/#options","text":"ground_truth_map: A dictionary representing ground truth with filenames as key and a list of duplicate filenames as value. retrieved_map: A dictionary representing retrieved duplicates with filenames as key and a list of retrieved duplicate filenames as value. metric can take one of the following values: 'map' 'ndcg' 'jaccard' 'classification': Returns per class precision, recall, f1-score, support 'all' (default, returns all the above metrics)","title":"Options"},{"location":"tutorials/evaluating_performance/#considerations","text":"Presently, the ground truth map should be prepared manually by the user. Symmetric relations between duplicates must be represented in the ground truth map. If an image i is a duplicate for image j , then j must also be represented as a duplicate of i . Absence of symmetric relations will lead to an exception. Both the ground_truth_map and retrieved_map must have the same keys.","title":"Considerations"},{"location":"tutorials/feature_generation/","text":"Feature generation It might be desirable to only generate the hashes/cnn encodings for a given image or all images in the directory instead of directly deduplicating using find_duplicates method. Features can be generated for a directory of images or for a single image: Feature generation for all images in a directory Feature generation for a single image Feature generation for all images in a directory To generate encodings for all images in an image directory encode_images function can be used. The general api for using encode_images is: from imagededup.methods import < method - name > method_object = < method - name > () encodings = method_object . encode_images ( image_dir = 'path/to/image/directory' ) where the returned variable encodings is a dictionary mapping image file names to corresponding encoding: { 'image1.jpg' : < feature - image - 1 > , 'image2.jpg' : < feature - image - 2 > , .. } For hashing algorithms, the features are 64 bit hashes represented as 16 character hexadecimal strings. For cnn, the features are numpy array with shape (1, 1024). Considerations If an image in the image directory can't be loaded, no features are generated for the image. Hence, there is no entry for the image in the returned encodings dictionary. Examples Generating features using Difference hash, from imagededup.methods import DHash dhasher = DHash () encodings = dhasher . encode_images ( image_dir = 'path/to/image/directory' ) Feature generation for a single image To generate encodings for a single image encode_image function can be used. The general api for using encode_image is: from imagededup.methods import < method - name > method_object = < method - name > () encoding = method_object . encode_image ( image_file = 'path/to/image/file' ) where the returned variable encoding is either a hexadecimal string if a hashing method is used or a (1, 1024) numpy array if cnn is used. Options image_file: Optional, path to the image file for which encodings are to be generated. image_array: Optional, used instead of image_file attribute. A numpy array representing the image. Considerations If the image can't be loaded, no features are generated for the image and None is returned. Examples Generating features using Difference hash, from imagededup.methods import DHash dhasher = DHash () encoding = dhasher . encode_image ( image_file = 'path/to/image/file' )","title":"Feature generation"},{"location":"tutorials/feature_generation/#feature-generation","text":"It might be desirable to only generate the hashes/cnn encodings for a given image or all images in the directory instead of directly deduplicating using find_duplicates method. Features can be generated for a directory of images or for a single image: Feature generation for all images in a directory Feature generation for a single image","title":"Feature generation"},{"location":"tutorials/feature_generation/#feature-generation-for-all-images-in-a-directory","text":"To generate encodings for all images in an image directory encode_images function can be used. The general api for using encode_images is: from imagededup.methods import < method - name > method_object = < method - name > () encodings = method_object . encode_images ( image_dir = 'path/to/image/directory' ) where the returned variable encodings is a dictionary mapping image file names to corresponding encoding: { 'image1.jpg' : < feature - image - 1 > , 'image2.jpg' : < feature - image - 2 > , .. } For hashing algorithms, the features are 64 bit hashes represented as 16 character hexadecimal strings. For cnn, the features are numpy array with shape (1, 1024).","title":"Feature generation for all images in a directory"},{"location":"tutorials/feature_generation/#considerations","text":"If an image in the image directory can't be loaded, no features are generated for the image. Hence, there is no entry for the image in the returned encodings dictionary.","title":"Considerations"},{"location":"tutorials/feature_generation/#examples","text":"Generating features using Difference hash, from imagededup.methods import DHash dhasher = DHash () encodings = dhasher . encode_images ( image_dir = 'path/to/image/directory' )","title":"Examples"},{"location":"tutorials/feature_generation/#feature-generation-for-a-single-image","text":"To generate encodings for a single image encode_image function can be used. The general api for using encode_image is: from imagededup.methods import < method - name > method_object = < method - name > () encoding = method_object . encode_image ( image_file = 'path/to/image/file' ) where the returned variable encoding is either a hexadecimal string if a hashing method is used or a (1, 1024) numpy array if cnn is used.","title":"Feature generation for a single image"},{"location":"tutorials/feature_generation/#options","text":"image_file: Optional, path to the image file for which encodings are to be generated. image_array: Optional, used instead of image_file attribute. A numpy array representing the image.","title":"Options"},{"location":"tutorials/feature_generation/#considerations_1","text":"If the image can't be loaded, no features are generated for the image and None is returned.","title":"Considerations"},{"location":"tutorials/feature_generation/#examples_1","text":"Generating features using Difference hash, from imagededup.methods import DHash dhasher = DHash () encoding = dhasher . encode_image ( image_file = 'path/to/image/file' )","title":"Examples"},{"location":"tutorials/finding_duplicates/","text":"Finding duplicates There are two methods available to perform deduplication: find_duplicates() find_duplicates_to_remove() find_duplicates() To deduplicate an image directory, the general api is: from imagededup.methods import < method - name > method_object = < method - name > () duplicates = method_object . find_duplicates ( image_dir = 'path/to/image/directory' , < threshold - parameter - value > ) where the returned variable duplicates is a dictionary with the following content: { 'image1.jpg' : [ 'image1_duplicate1.jpg' , 'image1_duplicate2.jpg' ], 'image2.jpg' : [..], .. } Each key in the duplicates dictionary corresponds to a file in the image directory passed to the image_dir parameter of the find_duplicates function. The value is a list of all file names in the image directory that were found to be duplicates for the key file. Options image_dir : Optional, directory where all image files are present. encoding_map : Optional, used instead of image_dir attribute. Set it equal to the dictionary of file names and corresponding features (hashes/cnn encodings). The mentioned dictionary can be generated using the corresponding encode_images method. scores : Setting it to True returns the scores representing the hamming distance (for hashing) or cosine similarity (for cnn) of each of the duplicate file names from the key file. In this case, the returned 'duplicates' dictionary has the following content: { 'image1.jpg' : [( 'image1_duplicate1.jpg' , score ), ( 'image1_duplicate2.jpg' , score )], 'image2.jpg' : [..], .. } Each key in the duplicates dictionary corresponds to a file in the image directory passed to the image_dir parameter of the find_duplicates function. The value is a list of all tuples representing the file names and corresponding scores in the image directory that were found to be duplicates for the key file. outfile : Name of file to which the returned duplicates dictionary is to be written. None by default. threshold parameter: min_similarity_threshold for cnn method indicating the minimum amount of cosine similarity that should exist between the key image and a candidate image so that the candidate image can be considered as a duplicate for the key image. Should be a float between -1.0 and 1.0. Default value is 0.9. max_distance_threshold for hashing methods indicating the maximum amount of hamming distance that can exist between the key image and a candidate image so that the candidate image can be considered as a duplicate for the key image. Should be an int between 0 and 64. Default value is 10. Considerations The returned duplicates dictionary contains symmetric relationships i.e., if an image i is a duplicate of image j , then image j must also be a duplicate of image i . Let's say that the image directory only consists of images i and j , then the duplicates dictionary would have the following content: { 'i' : [ 'j' ], 'j' : [ 'i' ] } If an image in the image directory can't be loaded, no features are generated for the image. Hence, the image is disregarded for deduplication and has no entry in the returned duplicates dictionary. Examples To deduplicate an image directory using perceptual hashing, with a maximum allowed hamming distance of 12, scores returned along with duplicate filenames and the returned dictionary saved to file 'my_duplicates.json', use the following: from imagededup.methods import PHash phasher = PHash () duplicates = phasher . find_duplicates ( image_dir = 'path/to/image/directory' , max_distance_threshold = 12 , scores = True , outfile = 'my_duplicates.json' ) To deduplicate an image directory using cnn, with a minimum cosine similarity of 0.85, no scores returned and the returned dictionary saved to file 'my_duplicates.json', use the following: from imagededup.methods import CNN cnn_encoder = CNN () duplicates = cnn_encoder . find_duplicates ( image_dir = 'path/to/image/directory' , min_similarity_threshold = 0.85 , scores = False , outfile = 'my_duplicates.json' ) find_duplicates_to_remove() Returns a list of files in the image directory that are considered as duplicates. Does NOT remove the said files. The api is similar to find_duplicates function (except the score attribute in find_duplicates ). This function allows the return of a single list of file names in directory that are found to be duplicates. The general api for the method is as below: from imagededup.methods import < method - name > method_object = < method - name > () duplicates = method_object . find_duplicates_to_remove ( image_dir = 'path/to/image/directory' , < threshold - parameter - value > ) In this case, the returned variable duplicates is a list containing the name of image files that are found to be duplicates of some file in the directory: [ 'image1_duplicate1.jpg' , 'image1_duplicate2.jpg' ,.. ] Options image_dir : Optional, directory where all image files are present. encoding_map : Optional, used instead of image_dir attribute. Set it equal to the dictionary of file names and corresponding features (hashes/cnn encodings). The mentioned dictionary can be generated using the corresponding encode_images method. Each key in the 'duplicates' dictionary corresponds to a file in the image directory passed to the image_dir parameter of the find_duplicates function. The value is a list of all tuples representing the file names and corresponding scores in the image directory that were found to be duplicates for the key file. outfile : Name of file to which the returned duplicates dictionary is to be written. None by default. threshold parameter: min_similarity_threshold for cnn method indicating the minimum amount of cosine similarity that should exist between the key image and a candidate image so that the candidate image can be considered as a duplicate for the key image. Should be a float between -1.0 and 1.0. Default value is 0.9. max_distance_threshold for hashing methods indicating the maximum amount of hamming distance that can exist between the key image and a candidate image so that the candidate image can be considered as a duplicate for the key image. Should be an int between 0 and 64. Default value is 10. Considerations This method must be used with caution. The symmetric nature of duplicates imposes an issue of marking one image as duplicate and the other as original. Consider the following duplicates dictionary: { '1.jpg' : [ '2.jpg' ], '2.jpg' : [ '1.jpg' , '3.jpg' ], '3.jpg' : [ '2.jpg' ] } In this case, it is possible to remove only 2.jpg which leaves 1.jpg and 3.jpg as non-duplicates of each other. However, it is also possible to remove both 1.jpg and 3.jpg leaving only 2.jpg . The find_duplicates_to_remove method makes this decision based on the alphabetical sorting of filenames in the directory. In the above example, the filename 1.jpg appears alphabetically before 2.jpg . So, 1.jpg would be retained, while its duplicate, 2.jpg , would be marked as a duplicate. Once 2.jpg is marked as duplicate, its own found duplicates would be disregarded. Thus, 1.jpg and 3.jpg would not be considered as duplicates. So, the final return would be: [ '2.jpg' ] This leaves 1.jpg and 3.jpg as non-duplicates in the directory. If the user does not wish to impose this heuristic, it is advised to use find_duplicates function and use a custom heuristic to mark a file as duplicate. If an image in the image directory can't be loaded, no features are generated for the image. Hence, the image is disregarded for deduplication and has no entry in the returned duplicates dictionary. Examples To deduplicate an image directory using perceptual hashing, with a maximum allowed hamming distance of 12, and the returned list saved to file 'my_duplicates.json', use the following: from imagededup.methods import PHash phasher = PHash () duplicates = phasher . find_duplicates_to_remove ( image_dir = 'path/to/image/directory' , max_distance_threshold = 12 , outfile = 'my_duplicates.json' ) To deduplicate an image directory using cnn, with a minimum cosine similarity of 0.85 and the returned list saved to file 'my_duplicates.json', use the following: from imagededup.methods import CNN cnn_encoder = CNN () duplicates = cnn_encoder . find_duplicates_to_remove ( image_dir = 'path/to/image/directory' , min_similarity_threshold = 0.85 , outfile = 'my_duplicates.json' )","title":"Find duplicates"},{"location":"tutorials/finding_duplicates/#finding-duplicates","text":"There are two methods available to perform deduplication: find_duplicates() find_duplicates_to_remove()","title":"Finding duplicates"},{"location":"tutorials/finding_duplicates/#find_duplicates","text":"To deduplicate an image directory, the general api is: from imagededup.methods import < method - name > method_object = < method - name > () duplicates = method_object . find_duplicates ( image_dir = 'path/to/image/directory' , < threshold - parameter - value > ) where the returned variable duplicates is a dictionary with the following content: { 'image1.jpg' : [ 'image1_duplicate1.jpg' , 'image1_duplicate2.jpg' ], 'image2.jpg' : [..], .. } Each key in the duplicates dictionary corresponds to a file in the image directory passed to the image_dir parameter of the find_duplicates function. The value is a list of all file names in the image directory that were found to be duplicates for the key file.","title":"find_duplicates()"},{"location":"tutorials/finding_duplicates/#options","text":"image_dir : Optional, directory where all image files are present. encoding_map : Optional, used instead of image_dir attribute. Set it equal to the dictionary of file names and corresponding features (hashes/cnn encodings). The mentioned dictionary can be generated using the corresponding encode_images method. scores : Setting it to True returns the scores representing the hamming distance (for hashing) or cosine similarity (for cnn) of each of the duplicate file names from the key file. In this case, the returned 'duplicates' dictionary has the following content: { 'image1.jpg' : [( 'image1_duplicate1.jpg' , score ), ( 'image1_duplicate2.jpg' , score )], 'image2.jpg' : [..], .. } Each key in the duplicates dictionary corresponds to a file in the image directory passed to the image_dir parameter of the find_duplicates function. The value is a list of all tuples representing the file names and corresponding scores in the image directory that were found to be duplicates for the key file. outfile : Name of file to which the returned duplicates dictionary is to be written. None by default. threshold parameter: min_similarity_threshold for cnn method indicating the minimum amount of cosine similarity that should exist between the key image and a candidate image so that the candidate image can be considered as a duplicate for the key image. Should be a float between -1.0 and 1.0. Default value is 0.9. max_distance_threshold for hashing methods indicating the maximum amount of hamming distance that can exist between the key image and a candidate image so that the candidate image can be considered as a duplicate for the key image. Should be an int between 0 and 64. Default value is 10.","title":"Options"},{"location":"tutorials/finding_duplicates/#considerations","text":"The returned duplicates dictionary contains symmetric relationships i.e., if an image i is a duplicate of image j , then image j must also be a duplicate of image i . Let's say that the image directory only consists of images i and j , then the duplicates dictionary would have the following content: { 'i' : [ 'j' ], 'j' : [ 'i' ] } If an image in the image directory can't be loaded, no features are generated for the image. Hence, the image is disregarded for deduplication and has no entry in the returned duplicates dictionary.","title":"Considerations"},{"location":"tutorials/finding_duplicates/#examples","text":"To deduplicate an image directory using perceptual hashing, with a maximum allowed hamming distance of 12, scores returned along with duplicate filenames and the returned dictionary saved to file 'my_duplicates.json', use the following: from imagededup.methods import PHash phasher = PHash () duplicates = phasher . find_duplicates ( image_dir = 'path/to/image/directory' , max_distance_threshold = 12 , scores = True , outfile = 'my_duplicates.json' ) To deduplicate an image directory using cnn, with a minimum cosine similarity of 0.85, no scores returned and the returned dictionary saved to file 'my_duplicates.json', use the following: from imagededup.methods import CNN cnn_encoder = CNN () duplicates = cnn_encoder . find_duplicates ( image_dir = 'path/to/image/directory' , min_similarity_threshold = 0.85 , scores = False , outfile = 'my_duplicates.json' )","title":"Examples"},{"location":"tutorials/finding_duplicates/#find_duplicates_to_remove","text":"Returns a list of files in the image directory that are considered as duplicates. Does NOT remove the said files. The api is similar to find_duplicates function (except the score attribute in find_duplicates ). This function allows the return of a single list of file names in directory that are found to be duplicates. The general api for the method is as below: from imagededup.methods import < method - name > method_object = < method - name > () duplicates = method_object . find_duplicates_to_remove ( image_dir = 'path/to/image/directory' , < threshold - parameter - value > ) In this case, the returned variable duplicates is a list containing the name of image files that are found to be duplicates of some file in the directory: [ 'image1_duplicate1.jpg' , 'image1_duplicate2.jpg' ,.. ]","title":"find_duplicates_to_remove()"},{"location":"tutorials/finding_duplicates/#options_1","text":"image_dir : Optional, directory where all image files are present. encoding_map : Optional, used instead of image_dir attribute. Set it equal to the dictionary of file names and corresponding features (hashes/cnn encodings). The mentioned dictionary can be generated using the corresponding encode_images method. Each key in the 'duplicates' dictionary corresponds to a file in the image directory passed to the image_dir parameter of the find_duplicates function. The value is a list of all tuples representing the file names and corresponding scores in the image directory that were found to be duplicates for the key file. outfile : Name of file to which the returned duplicates dictionary is to be written. None by default. threshold parameter: min_similarity_threshold for cnn method indicating the minimum amount of cosine similarity that should exist between the key image and a candidate image so that the candidate image can be considered as a duplicate for the key image. Should be a float between -1.0 and 1.0. Default value is 0.9. max_distance_threshold for hashing methods indicating the maximum amount of hamming distance that can exist between the key image and a candidate image so that the candidate image can be considered as a duplicate for the key image. Should be an int between 0 and 64. Default value is 10.","title":"Options"},{"location":"tutorials/finding_duplicates/#considerations_1","text":"This method must be used with caution. The symmetric nature of duplicates imposes an issue of marking one image as duplicate and the other as original. Consider the following duplicates dictionary: { '1.jpg' : [ '2.jpg' ], '2.jpg' : [ '1.jpg' , '3.jpg' ], '3.jpg' : [ '2.jpg' ] } In this case, it is possible to remove only 2.jpg which leaves 1.jpg and 3.jpg as non-duplicates of each other. However, it is also possible to remove both 1.jpg and 3.jpg leaving only 2.jpg . The find_duplicates_to_remove method makes this decision based on the alphabetical sorting of filenames in the directory. In the above example, the filename 1.jpg appears alphabetically before 2.jpg . So, 1.jpg would be retained, while its duplicate, 2.jpg , would be marked as a duplicate. Once 2.jpg is marked as duplicate, its own found duplicates would be disregarded. Thus, 1.jpg and 3.jpg would not be considered as duplicates. So, the final return would be: [ '2.jpg' ] This leaves 1.jpg and 3.jpg as non-duplicates in the directory. If the user does not wish to impose this heuristic, it is advised to use find_duplicates function and use a custom heuristic to mark a file as duplicate. If an image in the image directory can't be loaded, no features are generated for the image. Hence, the image is disregarded for deduplication and has no entry in the returned duplicates dictionary.","title":"Considerations"},{"location":"tutorials/finding_duplicates/#examples_1","text":"To deduplicate an image directory using perceptual hashing, with a maximum allowed hamming distance of 12, and the returned list saved to file 'my_duplicates.json', use the following: from imagededup.methods import PHash phasher = PHash () duplicates = phasher . find_duplicates_to_remove ( image_dir = 'path/to/image/directory' , max_distance_threshold = 12 , outfile = 'my_duplicates.json' ) To deduplicate an image directory using cnn, with a minimum cosine similarity of 0.85 and the returned list saved to file 'my_duplicates.json', use the following: from imagededup.methods import CNN cnn_encoder = CNN () duplicates = cnn_encoder . find_duplicates_to_remove ( image_dir = 'path/to/image/directory' , min_similarity_threshold = 0.85 , outfile = 'my_duplicates.json' )","title":"Examples"},{"location":"tutorials/plotting_duplicates/","text":"Plotting duplicates of an image Once a duplicate dictionary corresponding to an image directory has been obtained (using find_duplicates ), duplicates for an image can be plotted using plot_duplicates method as below: from imagededup.utils import plot_duplicates plot_duplicates ( image_dir , duplicate_map , filename ) where filename is the file for which duplicates are to be plotted. Options image_dir : Directory where all image files are present. duplicate_map : A dictionary representing retrieved duplicates with filenames as key and a list of retrieved duplicate filenames as value. A duplicate_map with scores can also be passed (obtained from find_duplicates function with scores attribute set to True). filename : Image file name for which duplicates are to be plotted. outfile : Name of the file the plot should be saved to. None by default. The output looks as below:","title":"Plotting duplicates"},{"location":"tutorials/plotting_duplicates/#plotting-duplicates-of-an-image","text":"Once a duplicate dictionary corresponding to an image directory has been obtained (using find_duplicates ), duplicates for an image can be plotted using plot_duplicates method as below: from imagededup.utils import plot_duplicates plot_duplicates ( image_dir , duplicate_map , filename ) where filename is the file for which duplicates are to be plotted.","title":"Plotting duplicates of an image"},{"location":"tutorials/plotting_duplicates/#options","text":"image_dir : Directory where all image files are present. duplicate_map : A dictionary representing retrieved duplicates with filenames as key and a list of retrieved duplicate filenames as value. A duplicate_map with scores can also be passed (obtained from find_duplicates function with scores attribute set to True). filename : Image file name for which duplicates are to be plotted. outfile : Name of the file the plot should be saved to. None by default. The output looks as below:","title":"Options"},{"location":"utils/data_generator/","text":"class DataGenerator Class inherits from Keras Sequence base object, allows to use multiprocessing in .fit_generator. Attributes image_dir : Path of image directory. batch_size : Number of images per batch. basenet_preprocess : Basenet specific preprocessing function. target_size : Dimensions that images get resized into when loaded. __init__ def __init__ ( image_dir , batch_size , basenet_preprocess , target_size ) Init DataGenerator object. on_epoch_end def on_epoch_end () Method called at the end of every epoch. __len__ def __len__ () Number of batches in the Sequence. __getitem__ def __getitem__ ( index ) Get batch at position index .","title":"Data generator"},{"location":"utils/data_generator/#class-datagenerator","text":"Class inherits from Keras Sequence base object, allows to use multiprocessing in .fit_generator.","title":"class DataGenerator"},{"location":"utils/data_generator/#attributes","text":"image_dir : Path of image directory. batch_size : Number of images per batch. basenet_preprocess : Basenet specific preprocessing function. target_size : Dimensions that images get resized into when loaded.","title":"Attributes"},{"location":"utils/data_generator/#9595init9595","text":"def __init__ ( image_dir , batch_size , basenet_preprocess , target_size ) Init DataGenerator object.","title":"__init__"},{"location":"utils/data_generator/#on95epoch95end","text":"def on_epoch_end () Method called at the end of every epoch.","title":"on_epoch_end"},{"location":"utils/data_generator/#9595len9595","text":"def __len__ () Number of batches in the Sequence.","title":"__len__"},{"location":"utils/data_generator/#9595getitem9595","text":"def __getitem__ ( index ) Get batch at position index .","title":"__getitem__"},{"location":"utils/general_utils/","text":"get_files_to_remove def get_files_to_remove ( duplicates ) Get a list of files to remove. Args duplicates : A dictionary with file name as key and a list of duplicate file names as value. Returns save_json def save_json ( results , filename ) Save results with a filename. Args results : Dictionary of results to be saved. filename : Name of the file to be saved. parallelise def parallelise ( function , data )","title":"General utils"},{"location":"utils/general_utils/#get95files95to95remove","text":"def get_files_to_remove ( duplicates ) Get a list of files to remove.","title":"get_files_to_remove"},{"location":"utils/general_utils/#args","text":"duplicates : A dictionary with file name as key and a list of duplicate file names as value.","title":"Args"},{"location":"utils/general_utils/#returns","text":"","title":"Returns"},{"location":"utils/general_utils/#save95json","text":"def save_json ( results , filename ) Save results with a filename.","title":"save_json"},{"location":"utils/general_utils/#args_1","text":"results : Dictionary of results to be saved. filename : Name of the file to be saved.","title":"Args"},{"location":"utils/general_utils/#parallelise","text":"def parallelise ( function , data )","title":"parallelise"},{"location":"utils/image_utils/","text":"preprocess_image def preprocess_image ( image , target_size , grayscale ) Take as input an image as numpy array or Pillow format. Returns an array version of optionally resized and grayed image. Args image : numpy array or a pillow image. target_size : Size to resize the input image to. grayscale : A boolean indicating whether to grayscale the image. Returns load_image def load_image ( image_file , target_size , grayscale , img_formats ) Load an image given its path. Returns an array version of optionally resized and grayed image. Only allows images of types described by img_formats argument. Args image_file : Path to the image file. target_size : Size to resize the input image to. grayscale : A boolean indicating whether to grayscale the image. img_formats : List of allowed image formats that can be loaded.","title":"Image utils"},{"location":"utils/image_utils/#preprocess95image","text":"def preprocess_image ( image , target_size , grayscale ) Take as input an image as numpy array or Pillow format. Returns an array version of optionally resized and grayed image.","title":"preprocess_image"},{"location":"utils/image_utils/#args","text":"image : numpy array or a pillow image. target_size : Size to resize the input image to. grayscale : A boolean indicating whether to grayscale the image.","title":"Args"},{"location":"utils/image_utils/#returns","text":"","title":"Returns"},{"location":"utils/image_utils/#load95image","text":"def load_image ( image_file , target_size , grayscale , img_formats ) Load an image given its path. Returns an array version of optionally resized and grayed image. Only allows images of types described by img_formats argument.","title":"load_image"},{"location":"utils/image_utils/#args_1","text":"image_file : Path to the image file. target_size : Size to resize the input image to. grayscale : A boolean indicating whether to grayscale the image. img_formats : List of allowed image formats that can be loaded.","title":"Args"},{"location":"utils/logger/","text":"return_logger def return_logger ( name , log_dir )","title":"Logger"},{"location":"utils/logger/#return95logger","text":"def return_logger ( name , log_dir )","title":"return_logger"},{"location":"utils/plotter/","text":"plot_duplicates def plot_duplicates ( image_dir , duplicate_map , filename , outfile ) Given filename for an image, plot duplicates along with the original image using the duplicate map obtained using find_duplicates method. Args image_dir : image directory where all files in duplicate_map are present. duplicate_map : mapping of filename to found duplicates (could be with or without scores). filename : Name of the file for which duplicates are to be plotted, must be a key in the duplicate_map outfile : Name of the file to save the plot. Example usage from imagededup.utils import plot_duplicates plot_duplicates ( image_dir = 'path/to/image/directory' , filename = 'path/to/image.jpg' ) OR plot_duplicates ( duplicate_map = duplicate_map , filename = 'path/to/image.jpg' )","title":"Plot duplicates"},{"location":"utils/plotter/#plot95duplicates","text":"def plot_duplicates ( image_dir , duplicate_map , filename , outfile ) Given filename for an image, plot duplicates along with the original image using the duplicate map obtained using find_duplicates method.","title":"plot_duplicates"},{"location":"utils/plotter/#args","text":"image_dir : image directory where all files in duplicate_map are present. duplicate_map : mapping of filename to found duplicates (could be with or without scores). filename : Name of the file for which duplicates are to be plotted, must be a key in the duplicate_map outfile : Name of the file to save the plot.","title":"Args"},{"location":"utils/plotter/#example-usage","text":"from imagededup.utils import plot_duplicates plot_duplicates ( image_dir = 'path/to/image/directory' , filename = 'path/to/image.jpg' ) OR plot_duplicates ( duplicate_map = duplicate_map , filename = 'path/to/image.jpg' )","title":"Example usage"}]}