{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imagehash import phash, dhash, average_hash\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import FunctionType\n",
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "from copy import deepcopy\n",
    "from numpy import array\n",
    "\n",
    "class Dataset:\n",
    "    \"\"\"\n",
    "    Class wrapper to instantiate a Dataset object composing of a subset of test images\n",
    "    and a smaller fraction of images that are used as queries to test search and retrieval.\n",
    "    Object contains hashed image fingerprints as well, however, hashing method is set by user.\n",
    "    \"\"\"\n",
    "    def __init__(self, path_to_queries: str, path_to_test: str) -> None:\n",
    "        #print(path_to_queries, path_to_test)\n",
    "        self.query_docs = self.load_image_set(path_to_queries)\n",
    "        self.test_docs = self.load_image_set(path_to_test)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_image_set(path: str) -> dict: \n",
    "        return {doc: os.path.join(path, doc) for doc in os.listdir(path) if doc.endswith('.jpg')}\n",
    "\n",
    "    \n",
    "class HashedDataset(Dataset):\n",
    "    def __init__(self, hashing_function: FunctionType, *args, **kwargs) -> None:\n",
    "        super(HashedDataset, self).__init__(*args, **kwargs)\n",
    "        self.hasher = hashing_function\n",
    "        # self.test_hashes = {doc: str(self.hasher(Image.open(self.test_docs[doc]))) for doc in self.test_docs}\n",
    "        # self.query_hashes = {doc: str(self.hasher(Image.open(self.query_docs[doc]))) for doc in self.query_docs}\n",
    "        self.fingerprint()\n",
    "        self.doc2hash = deepcopy(self.test_hashes)\n",
    "        #self.doc2hash.update(self.query_hashes)\n",
    "        self.hash2doc = {self.doc2hash[doc]: doc for doc in self.doc2hash}\n",
    "\n",
    "    \n",
    "    def fingerprint(self) -> None:\n",
    "        self.test_hashes = {doc: str(self.hasher(Image.open(self.test_docs[doc]))) for doc in self.test_docs}\n",
    "        self.query_hashes = {doc: str(self.hasher(Image.open(self.query_docs[doc]))) for doc in self.query_docs}\n",
    "        \n",
    "        \n",
    "    def get_hashes(self) -> dict:\n",
    "        return self.doc2hash\n",
    "\n",
    "\n",
    "    def get_query_hashes(self) -> dict:\n",
    "        return self.query_hashes\n",
    "\n",
    "\n",
    "    def get_test_hashes(self) -> dict:\n",
    "        return self.test_hashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "hull = HashedDataset(\n",
    "    dhash,\n",
    "    '/Users/zubin.john/forge/image-dedup/Transformed_dataset/Query/',\n",
    "    '/Users/zubin.john/forge/image-dedup/Transformed_dataset/Retrieval/'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shelve\n",
    "\n",
    "class ResultSet:\n",
    "    \"\"\"In order to retrieve duplicate images an index needs to be built against which\n",
    "    search operations are run. The ResultSe Class serves as a search and retrieval\n",
    "    interface, essential for driving interfacing for downstream tasks.\n",
    "\n",
    "    Takes input dictionary of image hashes for which DB has to be created.\"\"\"\n",
    "    def __init__(self, index_save_path: str, candidates:dict, queries: dict) -> None:\n",
    "        self.db_path = f'{index_save_path}.db'\n",
    "        self.db = self.create_db_index(index_save_path)\n",
    "        self.populate_db(candidates)\n",
    "        self.fetch_nearest_neighbors(queries)\n",
    "        self.destroy_db_index()\n",
    "\n",
    "    @staticmethod\n",
    "    def create_db_index(path) -> shelve.DbfilenameShelf:\n",
    "        return shelve.open(path, writeback=True)\n",
    "\n",
    "\n",
    "    def refresh_db_buffer(self) -> shelve.DbfilenameShelf:\n",
    "        return shelve.open(self.db_path)\n",
    "\n",
    "\n",
    "    def populate_db(self, candidates: dict):\n",
    "        for each in candidates:\n",
    "            self.db[candidates[each]] = self.db.get(candidates[each], []) + [each]\n",
    "        # Close the shelf database\n",
    "        self.db.close()\n",
    "\n",
    "\n",
    "    def fetch_nearest_neighbors(self, queries) -> None:\n",
    "        self.db = self.refresh_db_buffer()\n",
    "        self.query_results = {query: self.db[queries[query]] for query in queries}\n",
    "        self.db.close()\n",
    "\n",
    "        \n",
    "    def destroy_db_index(self) -> None:\n",
    "        if self.query_results and os.path.exists(self.db_path):\n",
    "            os.remove(self.db_path)\n",
    "            \n",
    "    def retrieve_results(self):\n",
    "        return self.query_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashes = hull.get_hashes()\n",
    "queries = hull.get_query_hashes()\n",
    "\n",
    "res = ResultSet('imageset', hashes, queries).retrieve_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from types import FunctionType\n",
    "\n",
    "\n",
    "class EvalPerformance:\n",
    "    def __init__(self, dict_correct: dict, dict_retrieved: dict) -> None:\n",
    "        self.dict_correct = dict_correct # dict of correct retrievals for each query(= ground truth), {'1.jpg': 'correct_dup1.jpg'}\n",
    "        self.dict_retrieved = dict_retrieved # dict of all retrievals for each query, {'1.jpg': 'retrieval_1.jpg'}\n",
    "\n",
    "    @staticmethod\n",
    "    def avg_prec(correct_duplicates: list, retrieved_duplicates: list) -> float:\n",
    "        \"\"\"Input: (list of correct duplicates (i.e., ground truth), list of retrieved duplicates) for one single query\n",
    "        return: float representing average precision for one input query\"\"\"\n",
    "        if not len(retrieved_duplicates):\n",
    "            return 0.0\n",
    "        count_real_correct = len(correct_duplicates)\n",
    "        relevance = np.array([1 if i in correct_duplicates else 0 for i in retrieved_duplicates])\n",
    "        relevance_cumsum = np.cumsum(relevance)\n",
    "        prec_k = [relevance_cumsum[k] / (k + 1) for k in range(len(relevance))]\n",
    "        prec_and_relevance = [relevance[k] * prec_k[k] for k in range(len(relevance))]\n",
    "        avg_precision = np.sum(prec_and_relevance) / count_real_correct\n",
    "        return avg_precision\n",
    "\n",
    "    @staticmethod\n",
    "    def ndcg(correct_duplicates: list, retrieved_duplicates: list) -> float:\n",
    "        \"\"\"Input: (list of correct duplicates (i.e., ground truth), list of retrieved duplicates) for one single query\n",
    "                return: float representing Normalized discounted Cumulative Gain (NDCG) for one input query\"\"\"\n",
    "        if not len(retrieved_duplicates):\n",
    "            return 0.0\n",
    "        relevance = np.array([1 if i in correct_duplicates else 0 for i in retrieved_duplicates])\n",
    "        relevance_numerator = [2 ** (k) - 1 for k in relevance]\n",
    "        relevance_denominator = [np.log2(k + 2) for k in\n",
    "                                 range(len(relevance))]  # first value of denominator term should be 2\n",
    "\n",
    "        dcg_terms = [relevance_numerator[k] / relevance_denominator[k] for k in range(len(relevance))]\n",
    "        dcg_k = np.sum(dcg_terms)\n",
    "\n",
    "        # get #retrievals\n",
    "        # if #retrievals <= #ground truth retrievals, set score=1 for calculating idcg\n",
    "        # else score=1 for first #ground truth retrievals entries, score=0 for remaining positions\n",
    "\n",
    "        if len(dcg_terms) <= len(correct_duplicates):\n",
    "            ideal_dcg = np.sum([1 / np.log2(k + 2) for k in range(len(dcg_terms))])\n",
    "            ndcg = dcg_k / ideal_dcg\n",
    "        else:\n",
    "            ideal_dcg_terms = [1] * len(correct_duplicates) + [0] * (len(dcg_terms) - len(correct_duplicates))\n",
    "            ideal_dcg_numerator = [(2 ** ideal_dcg_terms[k]) - 1 for k in range(len(ideal_dcg_terms))]\n",
    "            ideal_dcg_denominator = [np.log2(k + 2) for k in range(len(ideal_dcg_terms))]\n",
    "            ideal_dcg = np.sum([ideal_dcg_numerator[k] / ideal_dcg_denominator[k] for k in range(len(ideal_dcg_numerator))])\n",
    "            ndcg = dcg_k / ideal_dcg\n",
    "        return ndcg\n",
    "\n",
    "    @staticmethod\n",
    "    def jaccard_similarity(correct_duplicates: list, retrieved_duplicates: list) -> float:\n",
    "        \"\"\"Input: (list of correct duplicates (i.e., ground truth), list of retrieved duplicates) for one single query\n",
    "                return: float representing jaccard similarity for one input query\"\"\"\n",
    "        if not len(retrieved_duplicates):\n",
    "            return 0.0\n",
    "        set_correct_duplicates = set(correct_duplicates)\n",
    "        set_retrieved_duplicates = set(retrieved_duplicates)\n",
    "\n",
    "        intersection_dups = set_retrieved_duplicates.intersection(set_correct_duplicates)\n",
    "        union_dups = set_retrieved_duplicates.union(set_correct_duplicates)\n",
    "\n",
    "        jacc_sim = len(intersection_dups) / len(union_dups)\n",
    "        return jacc_sim\n",
    "\n",
    "    def mean_all_func(self, metric_func: FunctionType) -> float:\n",
    "        \"\"\"Input: metric function on which mean is to be calculated across all queries\n",
    "                return: float representing mean of the metric across all queries\"\"\"\n",
    "        all_metrics = []\n",
    "        for k in self.dict_correct.keys():\n",
    "            all_metrics.append(metric_func(self.dict_correct[k], self.dict_retrieved[k]))\n",
    "        return np.mean(all_metrics)\n",
    "\n",
    "    def get_all_metrics(self, save: bool=True) -> dict:\n",
    "        \"\"\"Input: Save flag indicating whether the dictionary below should be saved\n",
    "        return: dictionary of all mean metrics\"\"\"\n",
    "        dict_average_metrics = {\n",
    "            'MAP': self.mean_all_func(self.avg_prec),\n",
    "            'NDCG': self.mean_all_func(self.ndcg),\n",
    "            'Jaccard': self.mean_all_func(self.jaccard_similarity)\n",
    "        }\n",
    "\n",
    "        if save:\n",
    "            with open('all_average_metrics.pkl', 'wb') as f:\n",
    "                pickle.dump(dict_average_metrics, f)\n",
    "        return dict_average_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('/Users/zubin.john/forge/image-dedup/Transformed_dataset/ground_truth_transformed.pkl', 'rb') as rb:\n",
    "    correct_dict = pickle.load(rb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "e1 = EvalPerformance(correct_dict, res)\n",
    "e1.get_all_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Unit test\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "x = np.array(Image.open('/Users/zubin.john/forge/image-dedup/Transformed_dataset/Retrieval/ukbench04754_vflip.jpg'))\n",
    "y = Image.open('/Users/zubin.john/forge/image-dedup/Transformed_dataset/Retrieval/ukbench04754_vflip.jpg')           \n",
    "\n",
    "assert dhash(Image.fromarray(x)) == dhash(y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
